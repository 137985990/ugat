# curriculum_learning.py - è¯¾ç¨‹å­¦ä¹ æ¨¡å—

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Callable, Optional
from torch.utils.data import DataLoader, Subset
import random
from enum import Enum

class DifficultyMetric(Enum):
    """éš¾åº¦åº¦é‡æ–¹å¼"""
    SEQUENCE_LENGTH = "seq_length"      # åºåˆ—é•¿åº¦
    MISSING_RATIO = "missing_ratio"     # ç¼ºå¤±æ¯”ä¾‹
    NOISE_LEVEL = "noise_level"         # å™ªå£°æ°´å¹³
    LABEL_COMPLEXITY = "label_complex"  # æ ‡ç­¾å¤æ‚åº¦
    RECONSTRUCTION_ERROR = "recon_error" # é‡å»ºè¯¯å·®

class CurriculumScheduler:
    """è¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨"""
    
    def __init__(self, total_epochs: int, 
                 difficulty_metric: DifficultyMetric = DifficultyMetric.MISSING_RATIO,
                 curriculum_type: str = "linear"):
        """
        Args:
            total_epochs: æ€»è®­ç»ƒè½®æ•°
            difficulty_metric: éš¾åº¦åº¦é‡æ–¹å¼
            curriculum_type: è¯¾ç¨‹ç±»å‹ ["linear", "exponential", "step", "adaptive"]
        """
        self.total_epochs = total_epochs
        self.difficulty_metric = difficulty_metric
        self.curriculum_type = curriculum_type
        self.current_epoch = 0
        
        # éš¾åº¦çº§åˆ«å®šä¹‰
        self.difficulty_levels = {
            DifficultyMetric.SEQUENCE_LENGTH: [64, 128, 192, 256, 320],  # ä»çŸ­åˆ°é•¿
            DifficultyMetric.MISSING_RATIO: [0.1, 0.3, 0.5, 0.7, 0.9], # ä»å°‘ç¼ºå¤±åˆ°å¤šç¼ºå¤±
            DifficultyMetric.NOISE_LEVEL: [0.0, 0.1, 0.2, 0.3, 0.5],   # ä»æ— å™ªå£°åˆ°é«˜å™ªå£°
            DifficultyMetric.LABEL_COMPLEXITY: [0, 1, 2, 3, 4],         # ä»ç®€å•åˆ°å¤æ‚
        }
        
        # æ€§èƒ½å†å²è®°å½•
        self.performance_history = []
        self.difficulty_history = []
    
    def get_current_difficulty(self) -> float:
        """è·å–å½“å‰éš¾åº¦çº§åˆ« (0-1)"""
        progress = self.current_epoch / self.total_epochs
        
        if self.curriculum_type == "linear":
            return progress
        elif self.curriculum_type == "exponential":
            return progress ** 2
        elif self.curriculum_type == "step":
            # åˆ†é˜¶æ®µæå‡
            if progress < 0.3:
                return 0.2
            elif progress < 0.6:
                return 0.5
            elif progress < 0.8:
                return 0.8
            else:
                return 1.0
        elif self.curriculum_type == "adaptive":
            # æ ¹æ®æ€§èƒ½è‡ªé€‚åº”è°ƒæ•´
            return self._adaptive_difficulty()
        else:
            return progress
    
    def _adaptive_difficulty(self) -> float:
        """è‡ªé€‚åº”éš¾åº¦è°ƒæ•´"""
        if len(self.performance_history) < 3:
            return 0.2  # åˆå§‹ç®€å•
        
        # æ£€æŸ¥æœ€è¿‘3è½®çš„æ€§èƒ½è¶‹åŠ¿
        recent_performance = self.performance_history[-3:]
        avg_performance = np.mean(recent_performance)
        
        # å¦‚æœæ€§èƒ½ç¨³å®šä¸”è¾ƒå¥½ï¼Œå¢åŠ éš¾åº¦
        if avg_performance > 0.8 and np.std(recent_performance) < 0.1:
            current_difficulty = self.difficulty_history[-1] if self.difficulty_history else 0.2
            return min(1.0, current_difficulty + 0.1)
        
        # å¦‚æœæ€§èƒ½ä¸‹é™ï¼Œé™ä½éš¾åº¦
        elif len(self.performance_history) >= 2 and self.performance_history[-1] < self.performance_history[-2] - 0.1:
            current_difficulty = self.difficulty_history[-1] if self.difficulty_history else 0.5
            return max(0.1, current_difficulty - 0.1)
        
        # å¦åˆ™ä¿æŒå½“å‰éš¾åº¦
        return self.difficulty_history[-1] if self.difficulty_history else 0.2
    
    def update_performance(self, performance: float):
        """æ›´æ–°æ€§èƒ½è®°å½•"""
        self.performance_history.append(performance)
        self.difficulty_history.append(self.get_current_difficulty())
        
        # ä¿æŒå†å²è®°å½•é•¿åº¦
        if len(self.performance_history) > 20:
            self.performance_history.pop(0)
            self.difficulty_history.pop(0)
    
    def step_epoch(self):
        """æ›´æ–°epoch"""
        self.current_epoch += 1

class CurriculumDataset:
    """è¯¾ç¨‹å­¦ä¹ æ•°æ®é›†"""
    
    def __init__(self, base_dataset, curriculum_scheduler: CurriculumScheduler):
        self.base_dataset = base_dataset
        self.scheduler = curriculum_scheduler
        self.sample_difficulties = self._compute_sample_difficulties()
    
    def _compute_sample_difficulties(self) -> List[float]:
        """è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„éš¾åº¦"""
        difficulties = []
        
        for i in range(len(self.base_dataset)):
            try:
                sample = self.base_dataset[i]
                difficulty = self._compute_single_difficulty(sample, i)
                difficulties.append(difficulty)
            except:
                difficulties.append(0.5)  # é»˜è®¤ä¸­ç­‰éš¾åº¦
        
        return difficulties
    
    def _compute_single_difficulty(self, sample, idx: int) -> float:
        """è®¡ç®—å•ä¸ªæ ·æœ¬éš¾åº¦"""
        metric = self.scheduler.difficulty_metric
        
        if metric == DifficultyMetric.SEQUENCE_LENGTH:
            # åŸºäºåºåˆ—é•¿åº¦
            if hasattr(sample, '__len__'):
                seq_len = len(sample[0]) if isinstance(sample, (tuple, list)) else len(sample)
            else:
                seq_len = sample[0].shape[-1] if hasattr(sample[0], 'shape') else 320
            
            max_len = max(self.scheduler.difficulty_levels[metric])
            return min(1.0, seq_len / max_len)
        
        elif metric == DifficultyMetric.MISSING_RATIO:
            # åŸºäºç¼ºå¤±æ¯”ä¾‹ï¼ˆå¦‚æœæ ·æœ¬åŒ…å«maskä¿¡æ¯ï¼‰
            if isinstance(sample, (tuple, list)) and len(sample) >= 4:
                mask_info = sample[3]  # is_real_mask
                if hasattr(mask_info, 'sum'):
                    missing_ratio = 1.0 - (mask_info.sum().float() / mask_info.numel())
                    return missing_ratio.item()
            
            # éšæœºåˆ†é…ç¼ºå¤±æ¯”ä¾‹
            return random.random()
        
        elif metric == DifficultyMetric.NOISE_LEVEL:
            # åŸºäºæ•°æ®å™ªå£°æ°´å¹³
            if isinstance(sample, (tuple, list)):
                data = sample[0]
                if hasattr(data, 'std'):
                    noise_level = data.std().item()
                    return min(1.0, noise_level / 2.0)  # å½’ä¸€åŒ–
            return random.random()
        
        elif metric == DifficultyMetric.LABEL_COMPLEXITY:
            # åŸºäºæ ‡ç­¾å¤æ‚åº¦
            if isinstance(sample, (tuple, list)) and len(sample) >= 2:
                label = sample[1]
                if hasattr(label, 'item'):
                    label_val = label.item()
                    return label_val / 4.0  # å‡è®¾æœ€å¤§æ ‡ç­¾å€¼ä¸º4
            return random.random()
        
        return 0.5  # é»˜è®¤ä¸­ç­‰éš¾åº¦
    
    def get_curriculum_subset(self) -> Subset:
        """è·å–å½“å‰è¯¾ç¨‹éš¾åº¦å¯¹åº”çš„æ•°æ®å­é›†"""
        current_difficulty = self.scheduler.get_current_difficulty()
        
        # é€‰æ‹©éš¾åº¦ä¸è¶…è¿‡å½“å‰æ°´å¹³çš„æ ·æœ¬
        valid_indices = []
        for i, difficulty in enumerate(self.sample_difficulties):
            if difficulty <= current_difficulty + 0.1:  # å…è®¸å°å¹…è¶…å‡º
                valid_indices.append(i)
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
        if len(valid_indices) < 10:
            easy_indices = [i for i, d in enumerate(self.sample_difficulties) if d <= 0.5]
            valid_indices = easy_indices[:max(10, len(easy_indices)//2)]
        
        return Subset(self.base_dataset, valid_indices)

class CurriculumTrainer:
    """è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, model: nn.Module, base_dataset, 
                 curriculum_scheduler: CurriculumScheduler,
                 batch_size: int = 32):
        self.model = model
        self.base_dataset = base_dataset
        self.scheduler = curriculum_scheduler
        self.batch_size = batch_size
        
        # åˆ›å»ºè¯¾ç¨‹æ•°æ®é›†
        self.curriculum_dataset = CurriculumDataset(base_dataset, curriculum_scheduler)
        
        # æ€§èƒ½è¿½è¸ª
        self.training_log = {
            'epochs': [],
            'difficulties': [],
            'train_losses': [],
            'subset_sizes': []
        }
    
    def get_current_dataloader(self) -> DataLoader:
        """è·å–å½“å‰éš¾åº¦çš„æ•°æ®åŠ è½½å™¨"""
        current_subset = self.curriculum_dataset.get_curriculum_subset()
        
        return DataLoader(
            current_subset,
            batch_size=self.batch_size,
            shuffle=True,
            drop_last=False
        )
    
    def train_epoch(self, optimizer, criterion, device) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # è·å–å½“å‰è¯¾ç¨‹æ•°æ®
        dataloader = self.get_current_dataloader()
        
        total_loss = 0.0
        total_samples = 0
        
        current_difficulty = self.scheduler.get_current_difficulty()
        subset_size = len(dataloader.dataset)
        
        print(f"ğŸ“š Epoch {self.scheduler.current_epoch}: "
              f"éš¾åº¦ {current_difficulty:.2f}, "
              f"æ ·æœ¬æ•° {subset_size}")
        
        for batch_idx, batch_data in enumerate(dataloader):
            # è§£åŒ…æ‰¹æ¬¡æ•°æ®
            if isinstance(batch_data, (tuple, list)):
                inputs = batch_data[0].to(device)
                labels = batch_data[1].to(device) if len(batch_data) > 1 else None
            else:
                inputs = batch_data.to(device)
                labels = None
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­
            if labels is not None:
                outputs, logits = self.model(inputs)
                
                # è®¡ç®—æŸå¤±ï¼ˆé‡å»º + åˆ†ç±»ï¼‰
                recon_loss = criterion(outputs, inputs)
                if logits is not None and labels is not None:
                    ce_loss = nn.CrossEntropyLoss()(logits.unsqueeze(0), labels.unsqueeze(0))
                    loss = recon_loss + ce_loss
                else:
                    loss = recon_loss
            else:
                outputs = self.model(inputs)
                loss = criterion(outputs, inputs)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            total_samples += inputs.size(0)
        
        avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else float('inf')
        
        # æ›´æ–°è¯¾ç¨‹è°ƒåº¦å™¨
        # è¿™é‡Œç”¨è´ŸæŸå¤±ä½œä¸ºæ€§èƒ½æŒ‡æ ‡ï¼ˆæŸå¤±è¶Šä½ï¼Œæ€§èƒ½è¶Šå¥½ï¼‰
        performance = max(0.0, 1.0 - avg_loss)  # ç®€å•çš„æ€§èƒ½è½¬æ¢
        self.scheduler.update_performance(performance)
        self.scheduler.step_epoch()
        
        # è®°å½•è®­ç»ƒæ—¥å¿—
        self.training_log['epochs'].append(self.scheduler.current_epoch)
        self.training_log['difficulties'].append(current_difficulty)
        self.training_log['train_losses'].append(avg_loss)
        self.training_log['subset_sizes'].append(subset_size)
        
        return {
            'loss': avg_loss,
            'difficulty': current_difficulty,
            'subset_size': subset_size,
            'performance': performance
        }
    
    def plot_curriculum_progress(self, save_path: str = "curriculum_progress.png"):
        """ç»˜åˆ¶è¯¾ç¨‹å­¦ä¹ è¿›åº¦"""
        if not self.training_log['epochs']:
            return
        
        import matplotlib.pyplot as plt
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        epochs = self.training_log['epochs']
        
        # 1. éš¾åº¦è¿›åº¦
        ax1.plot(epochs, self.training_log['difficulties'], 'b-o', markersize=4)
        ax1.set_title('Curriculum Difficulty Progress')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Difficulty Level')
        ax1.grid(True, alpha=0.3)
        
        # 2. è®­ç»ƒæŸå¤±
        ax2.plot(epochs, self.training_log['train_losses'], 'r-o', markersize=4)
        ax2.set_title('Training Loss')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Loss')
        ax2.grid(True, alpha=0.3)
        
        # 3. æ•°æ®é›†å¤§å°å˜åŒ–
        ax3.plot(epochs, self.training_log['subset_sizes'], 'g-o', markersize=4)
        ax3.set_title('Training Subset Size')
        ax3.set_xlabel('Epoch')
        ax3.set_ylabel('Number of Samples')
        ax3.grid(True, alpha=0.3)
        
        # 4. éš¾åº¦vsæ€§èƒ½æ•£ç‚¹å›¾
        if len(self.scheduler.performance_history) > 0:
            ax4.scatter(self.training_log['difficulties'], 
                       self.scheduler.performance_history[-len(epochs):],
                       c=epochs, cmap='viridis', alpha=0.7)
            ax4.set_title('Difficulty vs Performance')
            ax4.set_xlabel('Difficulty Level')
            ax4.set_ylabel('Performance')
            ax4.grid(True, alpha=0.3)
            
            # æ·»åŠ é¢œè‰²æ¡
            cbar = plt.colorbar(ax4.collections[0], ax=ax4)
            cbar.set_label('Epoch')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š è¯¾ç¨‹å­¦ä¹ è¿›åº¦å›¾å·²ä¿å­˜: {save_path}")

def create_curriculum_trainer(model, dataset, config: Dict) -> CurriculumTrainer:
    """åˆ›å»ºè¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨çš„å·¥å‚å‡½æ•°"""
    
    # è§£æé…ç½®
    total_epochs = config.get('epochs', 100)
    difficulty_metric = DifficultyMetric(config.get('curriculum_metric', 'missing_ratio'))
    curriculum_type = config.get('curriculum_type', 'linear')
    batch_size = config.get('batch_size', 32)
    
    # åˆ›å»ºè°ƒåº¦å™¨
    scheduler = CurriculumScheduler(
        total_epochs=total_epochs,
        difficulty_metric=difficulty_metric,
        curriculum_type=curriculum_type
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CurriculumTrainer(
        model=model,
        base_dataset=dataset,
        curriculum_scheduler=scheduler,
        batch_size=batch_size
    )
    
    print(f"ğŸ“š è¯¾ç¨‹å­¦ä¹ è®­ç»ƒå™¨å·²åˆ›å»º:")
    print(f"  - æ€»è½®æ•°: {total_epochs}")
    print(f"  - éš¾åº¦åº¦é‡: {difficulty_metric.value}")
    print(f"  - è¯¾ç¨‹ç±»å‹: {curriculum_type}")
    print(f"  - æ•°æ®é›†å¤§å°: {len(dataset)}")
    
    return trainer
