# attention_visualizer.py - æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–æ¨¡å—

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import os
from datetime import datetime

class AttentionExtractor(nn.Module):
    """æ³¨æ„åŠ›æƒé‡æå–å™¨"""
    
    def __init__(self, model):
        super().__init__()
        self.model = model
        self.attention_maps = {}
        self.hooks = []
        self.register_hooks()
    
    def register_hooks(self):
        """æ³¨å†Œé’©å­å‡½æ•°æ¥æ•è·æ³¨æ„åŠ›æƒé‡"""
        def get_attention_hook(name):
            def hook(module, input, output):
                if hasattr(module, 'attention_weights') or len(output) > 1:
                    # å¦‚æœæ¨¡å—è¿”å›æ³¨æ„åŠ›æƒé‡
                    if isinstance(output, tuple) and len(output) > 1:
                        self.attention_maps[name] = output[1].detach().cpu()
                    elif hasattr(module, 'attention_weights'):
                        self.attention_maps[name] = module.attention_weights.detach().cpu()
            return hook
        
        # ä¸ºæ‰€æœ‰GATå±‚æ³¨å†Œé’©å­
        for name, module in self.model.named_modules():
            if 'gat' in name.lower() or 'attention' in name.lower():
                hook = module.register_forward_hook(get_attention_hook(name))
                self.hooks.append(hook)
    
    def remove_hooks(self):
        """ç§»é™¤æ‰€æœ‰é’©å­"""
        for hook in self.hooks:
            hook.remove()
        self.hooks = []
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­å¹¶æ”¶é›†æ³¨æ„åŠ›æƒé‡"""
        self.attention_maps.clear()
        output = self.model(x)
        return output, self.attention_maps

class AttentionVisualizer:
    """æ³¨æ„åŠ›å¯è§†åŒ–å™¨"""
    
    def __init__(self, save_dir: str = "attention_plots"):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    
    def plot_attention_matrix(self, attention_weights: torch.Tensor, 
                            layer_name: str, head_idx: int = 0,
                            save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶æ³¨æ„åŠ›æƒé‡çŸ©é˜µçƒ­åŠ›å›¾
        
        Args:
            attention_weights: [num_heads, num_edges] or [num_edges] æ³¨æ„åŠ›æƒé‡
            layer_name: å±‚åç§°
            head_idx: æ³¨æ„åŠ›å¤´ç´¢å¼•
            save_path: ä¿å­˜è·¯å¾„
        """
        if attention_weights.dim() == 2:
            # å¤šå¤´æ³¨æ„åŠ›
            weights = attention_weights[head_idx].numpy()
        else:
            # å•å¤´æ³¨æ„åŠ›
            weights = attention_weights.numpy()
        
        # é‡æ„ä¸ºçŸ©é˜µå½¢å¼ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å›¾ç»“æ„è°ƒæ•´ï¼‰
        seq_len = int(np.sqrt(len(weights))) if len(weights) > 0 else 1
        if seq_len * seq_len != len(weights):
            # å¦‚æœä¸æ˜¯å®Œå…¨å›¾ï¼Œä½¿ç”¨ç¨€ç–è¡¨ç¤º
            matrix = np.zeros((seq_len, seq_len))
            # ç®€åŒ–å¤„ç†ï¼šå°†æƒé‡åˆ†å¸ƒåˆ°å¯¹è§’çº¿é™„è¿‘
            for i, w in enumerate(weights[:seq_len]):
                if i < seq_len:
                    matrix[i, i] = w
        else:
            matrix = weights.reshape(seq_len, seq_len)
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(matrix, 
                   annot=True if seq_len <= 10 else False,
                   fmt='.3f',
                   cmap='Blues',
                   cbar_kws={'label': 'Attention Weight'})
        
        plt.title(f'{layer_name} - Head {head_idx} Attention Map', fontsize=14, fontweight='bold')
        plt.xlabel('Target Position', fontsize=12)
        plt.ylabel('Source Position', fontsize=12)
        
        if save_path is None:
            save_path = os.path.join(self.save_dir, f'{layer_name}_head{head_idx}_attention.png')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ’¾ æ³¨æ„åŠ›å›¾å·²ä¿å­˜: {save_path}")
    
    def plot_attention_distribution(self, attention_maps: Dict[str, torch.Tensor],
                                  save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶æ‰€æœ‰å±‚çš„æ³¨æ„åŠ›åˆ†å¸ƒç»Ÿè®¡
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.ravel()
        
        layer_names = list(attention_maps.keys())
        colors = plt.cm.Set3(np.linspace(0, 1, len(layer_names)))
        
        # 1. æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒç›´æ–¹å›¾
        axes[0].set_title('Attention Weight Distribution', fontsize=12, fontweight='bold')
        for i, (layer_name, weights) in enumerate(attention_maps.items()):
            if weights.numel() > 0:
                flat_weights = weights.flatten().numpy()
                axes[0].hist(flat_weights, bins=30, alpha=0.6, 
                           label=layer_name, color=colors[i], density=True)
        axes[0].set_xlabel('Attention Weight')
        axes[0].set_ylabel('Density')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # 2. æ³¨æ„åŠ›æƒé‡æ–¹å·®ï¼ˆè¡¡é‡æ³¨æ„åŠ›é›†ä¸­åº¦ï¼‰
        axes[1].set_title('Attention Concentration (Variance)', fontsize=12, fontweight='bold')
        variances = []
        for layer_name, weights in attention_maps.items():
            if weights.numel() > 0:
                var = weights.var().item()
                variances.append(var)
            else:
                variances.append(0)
        
        bars = axes[1].bar(range(len(layer_names)), variances, color=colors)
        axes[1].set_xticks(range(len(layer_names)))
        axes[1].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[1].set_ylabel('Variance')
        axes[1].grid(True, alpha=0.3, axis='y')
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, var in zip(bars, variances):
            height = bar.get_height()
            axes[1].text(bar.get_x() + bar.get_width()/2., height,
                        f'{var:.4f}', ha='center', va='bottom', fontsize=9)
        
        # 3. æ³¨æ„åŠ›æƒé‡æœ€å¤§å€¼ï¼ˆè¡¡é‡å³°å€¼å¼ºåº¦ï¼‰
        axes[2].set_title('Maximum Attention Weight', fontsize=12, fontweight='bold')
        max_weights = []
        for layer_name, weights in attention_maps.items():
            if weights.numel() > 0:
                max_w = weights.max().item()
                max_weights.append(max_w)
            else:
                max_weights.append(0)
        
        bars = axes[2].bar(range(len(layer_names)), max_weights, color=colors)
        axes[2].set_xticks(range(len(layer_names)))
        axes[2].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[2].set_ylabel('Max Weight')
        axes[2].grid(True, alpha=0.3, axis='y')
        
        # 4. æ³¨æ„åŠ›ç†µï¼ˆè¡¡é‡æ³¨æ„åŠ›åˆ†æ•£ç¨‹åº¦ï¼‰
        axes[3].set_title('Attention Entropy', fontsize=12, fontweight='bold')
        entropies = []
        for layer_name, weights in attention_maps.items():
            if weights.numel() > 0:
                # è®¡ç®—ç†µ
                probs = torch.softmax(weights.flatten(), dim=0)
                entropy = -(probs * torch.log(probs + 1e-8)).sum().item()
                entropies.append(entropy)
            else:
                entropies.append(0)
        
        bars = axes[3].bar(range(len(layer_names)), entropies, color=colors)
        axes[3].set_xticks(range(len(layer_names)))
        axes[3].set_xticklabels(layer_names, rotation=45, ha='right')
        axes[3].set_ylabel('Entropy')
        axes[3].grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        
        if save_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = os.path.join(self.save_dir, f'attention_analysis_{timestamp}.png')
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š æ³¨æ„åŠ›åˆ†æå›¾å·²ä¿å­˜: {save_path}")
    
    def plot_temporal_attention(self, attention_weights: torch.Tensor, 
                              time_steps: List[int], layer_name: str,
                              save_path: Optional[str] = None) -> None:
        """
        ç»˜åˆ¶æ—¶é—´æ­¥æ³¨æ„åŠ›æ¨¡å¼
        """
        if attention_weights.numel() == 0:
            return
        
        plt.figure(figsize=(12, 6))
        
        # å‡è®¾attention_weightsæ˜¯æ—¶é—´æ­¥ä¹‹é—´çš„æ³¨æ„åŠ›
        if attention_weights.dim() == 2:
            # å¤šå¤´ï¼šå–å¹³å‡
            weights = attention_weights.mean(dim=0).numpy()
        else:
            weights = attention_weights.numpy()
        
        # åˆ›å»ºæ—¶é—´æ­¥æ³¨æ„åŠ›å›¾
        seq_len = len(time_steps)
        if len(weights) >= seq_len:
            # é‡æ–°æ•´ç†ä¸ºæ—¶é—´æ­¥çŸ©é˜µ
            time_attention = weights[:seq_len]
            
            plt.subplot(1, 2, 1)
            plt.plot(time_steps, time_attention, 'b-o', linewidth=2, markersize=6)
            plt.title(f'{layer_name} - Temporal Attention Pattern')
            plt.xlabel('Time Step')
            plt.ylabel('Attention Weight')
            plt.grid(True, alpha=0.3)
            
            # æ³¨æ„åŠ›ç„¦ç‚¹åˆ†æ
            plt.subplot(1, 2, 2)
            focus_window = 5  # æ³¨æ„åŠ›ç„¦ç‚¹çª—å£
            smoothed = np.convolve(time_attention, np.ones(focus_window)/focus_window, mode='valid')
            plt.plot(time_steps[focus_window-1:], smoothed, 'r-', linewidth=2, label='Smoothed')
            plt.plot(time_steps, time_attention, 'b-', alpha=0.5, label='Original')
            plt.title(f'{layer_name} - Attention Focus')
            plt.xlabel('Time Step')
            plt.ylabel('Attention Weight')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path is None:
            save_path = os.path.join(self.save_dir, f'{layer_name}_temporal_attention.png')
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"â° æ—¶é—´æ³¨æ„åŠ›å›¾å·²ä¿å­˜: {save_path}")

def analyze_model_attention(model, sample_input: torch.Tensor, 
                          save_dir: str = "attention_analysis") -> Dict:
    """
    å®Œæ•´çš„æ¨¡å‹æ³¨æ„åŠ›åˆ†æ
    
    Args:
        model: è¦åˆ†æçš„æ¨¡å‹
        sample_input: æ ·æœ¬è¾“å…¥
        save_dir: ä¿å­˜ç›®å½•
    
    Returns:
        attention_info: æ³¨æ„åŠ›åˆ†æç»“æœ
    """
    model.eval()
    
    # åˆ›å»ºæ³¨æ„åŠ›æå–å™¨
    extractor = AttentionExtractor(model)
    visualizer = AttentionVisualizer(save_dir)
    
    try:
        with torch.no_grad():
            # æå–æ³¨æ„åŠ›æƒé‡
            output, attention_maps = extractor(sample_input)
            
            print(f"ğŸ” æå–åˆ° {len(attention_maps)} ä¸ªæ³¨æ„åŠ›å±‚çš„æƒé‡")
            
            # ç”Ÿæˆå„ç§å¯è§†åŒ–
            # 1. æ•´ä½“åˆ†æ
            visualizer.plot_attention_distribution(attention_maps)
            
            # 2. å„å±‚è¯¦ç»†çƒ­åŠ›å›¾
            for layer_name, weights in attention_maps.items():
                if weights.numel() > 0:
                    if weights.dim() == 2:  # å¤šå¤´
                        for head_idx in range(min(weights.size(0), 4)):  # æœ€å¤šæ˜¾ç¤º4ä¸ªå¤´
                            visualizer.plot_attention_matrix(weights, layer_name, head_idx)
                    else:  # å•å¤´
                        visualizer.plot_attention_matrix(weights, layer_name)
            
            # 3. æ—¶é—´åºåˆ—æ³¨æ„åŠ›ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
            if hasattr(sample_input, 'shape') and len(sample_input.shape) >= 2:
                time_steps = list(range(sample_input.shape[0]))  # å‡è®¾ç¬¬ä¸€ç»´æ˜¯æ—¶é—´
                for layer_name, weights in attention_maps.items():
                    if weights.numel() > 0:
                        visualizer.plot_temporal_attention(weights, time_steps, layer_name)
            
            # ç»Ÿè®¡ä¿¡æ¯
            attention_info = {
                'num_layers': len(attention_maps),
                'layer_stats': {}
            }
            
            for layer_name, weights in attention_maps.items():
                if weights.numel() > 0:
                    attention_info['layer_stats'][layer_name] = {
                        'shape': list(weights.shape),
                        'mean': weights.mean().item(),
                        'std': weights.std().item(),
                        'max': weights.max().item(),
                        'min': weights.min().item()
                    }
            
            print(f"ğŸ“Š æ³¨æ„åŠ›åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åœ¨: {save_dir}")
            return attention_info
            
    finally:
        # æ¸…ç†é’©å­
        extractor.remove_hooks()
