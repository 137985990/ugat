def set_seed(seed=42):
    import random
    import numpy as np
    import torch
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # ä¿è¯éƒ¨åˆ†åº“çš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

import collections
from simple_multimodal_integration import create_simple_multimodal_criterion
from enhanced_validation_integration import EnhancedValidationManager

def check_label_distribution(dataset):
    """
    æ£€æŸ¥å¹¶è¾“å‡ºæ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒå’Œæ‰€æœ‰æ ‡ç­¾ç§ç±»
    """
    label_counter = collections.Counter()
    all_labels = set()
    for i in range(len(dataset)):
        item = dataset[i]
        label = item[1]
        if hasattr(label, 'item'):
            label = label.item()
        label_counter[label] += 1
        all_labels.add(label)
    print("æ ‡ç­¾åˆ†å¸ƒ:", dict(label_counter))
    print("æ‰€æœ‰æ ‡ç­¾:", sorted(list(all_labels)))
    return label_counter, all_labels

def complete_need_with_model(model, dataset, device):
    """
    ç”¨æ¨¡å‹å¯¹æ•´ä¸ªæ•°æ®é›†çš„needé€šé“è¿›è¡Œè¡¥å…¨ï¼Œå¹¶å†™å›datasetï¼ˆåŠ¨æ€needæ›´æ–°ï¼‰ã€‚
    åªæ›´æ–°need_indicesæŒ‡å®šçš„é€šé“ã€‚
    """
    model.eval()
    from torch.utils.data import DataLoader
    import torch
    # æ”¯æŒè”åˆæ•°æ®é›†ï¼šæ¯ä¸ªæ ·æœ¬need_indiceså¯èƒ½ä¸åŒ
    from torch.utils.data import DataLoader
    import torch
    loader = DataLoader(dataset, batch_size=32)
    all_preds = []
    sample_base_idx = 0
    with torch.no_grad():
        for batch in tqdm(loader, desc="Complete need (dynamic)"):
            # å…¼å®¹è”åˆæ•°æ®é›† __getitem__ è¿”å› (x, label, mask_idx, is_real_mask, source) æˆ– (x, ...)
            if isinstance(batch, (tuple, list)) and len(batch) >= 1:
                batch_x = batch[0]
            else:
                batch_x = batch
            batch_x = batch_x.to(device)
            batch_size, C, T = batch_x.size()
            for i in range(batch_size):
                # è·å–æ¯ä¸ªæ ·æœ¬çš„need_indices
                if hasattr(dataset, 'need_indices') and isinstance(dataset.need_indices, list) and len(dataset.need_indices) == len(dataset):
                    need_idx = dataset.need_indices[sample_base_idx + i]
                else:
                    need_idx = getattr(dataset, 'need_indices', None)
                if not need_idx or len(need_idx) == 0:
                    # å…¼å®¹æ— needé€šé“çš„æ ·æœ¬
                    all_preds.append(torch.empty((0, T), device=batch_x.device))
                    continue
                masked = batch_x[i].clone()
                for idx in need_idx:
                    masked[idx, :] = 0
                window = masked.t()  # [T, C]
                out, _ = model(window)
                out_need = out[need_idx, :]  # [num_need, T]
                all_preds.append(out_need)
            sample_base_idx += batch_size
    # all_preds: list of [num_need, T]ï¼Œæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹
    if hasattr(dataset, 'update_need'):
        dataset.update_need(all_preds)

import yaml
import numpy as np
import pandas as pd
import os
import argparse
import yaml
import logging
from datetime import datetime

import torch
from tqdm import tqdm
from torch.utils.data import DataLoader, random_split
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.nn import MSELoss

from torch.utils.tensorboard.writer import SummaryWriter

from data import create_dataset_from_config
from model import TGATUNet
from model_optimized import OptimizedTGATUNet, create_optimized_model
from memory_optimizer import MemoryEfficientDataLoader, MemoryMonitor, create_memory_efficient_loader
from graph_cache import build_graph_cached
from unet_enhanced import UNetTGAT
from attention_visualizer import analyze_model_attention
from curriculum_learning import create_curriculum_trainer, CurriculumTrainer
from enhanced_validation_integration import EnhancedValidationManager


def complete_dataset_to_csv(model, dataset, mask_indices, device, save_path, dataset_modalities):
    """
    ç”¨æ¨¡å‹å¯¹æ•°æ®é›†è¿›è¡Œæ¨¡æ€è¡¥å…¨ï¼Œä¿å­˜è¡¥å…¨åçš„æ•°æ®ä¸ºCSV
    """
    model.eval()
    # 1. è¯»å–åŸå§‹csvï¼Œä¿ç•™blockå’Œlabelåˆ—ï¼Œè¡¥å…¨modalities
    import pandas as pd
    import os
    # æ¨æ–­åŸå§‹æ–‡ä»¶è·¯å¾„ï¼Œä¼˜å…ˆç”¨config.yamlé‡Œçš„data_files
    config_path = 'e:/NEW/V5/config.yaml'
    with open(config_path, 'r', encoding='utf-8') as f:
        cfg = yaml.safe_load(f)
    # å–data_filesç»å¯¹è·¯å¾„
    data_files = cfg.get('data_files', [])
    # å…¼å®¹ä¸åŒå‘½å
    def find_file(keyword):
        for path in data_files:
            if keyword.lower() in os.path.basename(path).lower():
                # æ”¯æŒç›¸å¯¹è·¯å¾„
                abs_path = os.path.abspath(os.path.join(os.path.dirname(config_path), path))
                if os.path.exists(abs_path):
                    return abs_path
        # å…œåº•ï¼šå°è¯•V5/Data/ä¸‹
        fallback = os.path.abspath(os.path.join(os.path.dirname(config_path), 'Data', f'{keyword}_original.csv'))
        if os.path.exists(fallback):
            return fallback
        # å…œåº•ï¼šå°è¯•NEW/Data/ä¸‹
        fallback2 = os.path.abspath(os.path.join('e:/NEW/Data', f'{keyword}_original.csv'))
        if os.path.exists(fallback2):
            return fallback2
        raise FileNotFoundError(f'æ‰¾ä¸åˆ°åŸå§‹csv: {keyword}')
    if 'FM' in save_path:
        orig_path = find_file('FM')
    elif 'OD' in save_path:
        orig_path = find_file('OD')
    elif 'MEFAR' in save_path:
        orig_path = find_file('MEFAR')
    else:
        raise ValueError('æ— æ³•æ¨æ–­åŸå§‹csvè·¯å¾„')
    orig_df = pd.read_csv(orig_path)
    # ç»Ÿä¸€åˆ—åå°å†™ã€å»ç©ºæ ¼
    # orig_df.columns = [c.strip().lower() for c in orig_df.columns]  # ä¿ç•™åŸå§‹å¤§å†™F
    block_col = 'block'
    label_col = 'f' if 'f' in orig_df.columns else orig_df.columns[1]  # é»˜è®¤ç¬¬2åˆ—ä¸ºlabel
    # ä¿è¯æ‰€æœ‰modalitieséƒ½åœ¨
    for m in dataset_modalities:
        if m not in orig_df.columns:
            orig_df[m] = 0.0
    # ä¿è¯é¡ºåºï¼šblock, label, 32ä¸ªmodalities
    out_cols = [block_col, label_col] + list(dataset_modalities)
    orig_df = orig_df[out_cols]

    # 2. ç”¨æ¨¡å‹æ»‘åŠ¨çª—å£è¡¥å…¨modalitieséƒ¨åˆ†ï¼ˆå¯¹æ¯ä¸ªæ—¶é—´ç‚¹åšé‡å å¹³å‡ï¼‰
    from torch.utils.data import DataLoader
    import torch
    window_size = dataset[0][0].shape[-1]  # å‡è®¾datasetæ¯ä¸ªæ ·æœ¬ä¸º [C, T]
    stride = 1  # æ»‘åŠ¨æ­¥é•¿ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    num_rows = len(orig_df)
    num_modalities = len(dataset_modalities)
    # ç”¨äºç´¯åŠ è¡¥å…¨ç»“æœå’Œè®¡æ•°
    completed_sum = np.zeros((num_rows, num_modalities), dtype=np.float32)
    completed_count = np.zeros((num_rows, num_modalities), dtype=np.float32)

    # æ„é€ æ»‘çª—ç´¢å¼•ï¼ˆå‡è®¾dataseté¡ºåºä¸åŸå§‹csvä¸€è‡´ï¼Œä¸”æ¯ä¸ªæ ·æœ¬ä¸ºä¸€ä¸ªæ»‘çª—ï¼‰
    loader = DataLoader(dataset, batch_size=32)
    row_idx = 0
    with torch.no_grad():
        for batch_idx, (batch, label, mask_idx, is_real_mask) in enumerate(tqdm(loader, desc="Complete (sliding window)")):
            batch = batch.to(device)
            masked, mask_idx = mask_channel(batch, mask_indices)
            batch_size, C, T = batch.size()
            for i in range(batch_size):
                window = masked[i].t()  # [T, C]
                out, _ = model(window)  # out: [C, T]
                out_np = out.cpu().numpy().T  # [T, C] -> [T, C]
                start = row_idx
                end = row_idx + T
                if end > num_rows:
                    break
                completed_sum[start:end, :] += out_np
                completed_count[start:end, :] += 1
                row_idx += stride
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾datasetæ˜¯é¡ºåºæ»‘çª—ï¼Œè‹¥ä¸æ˜¯éœ€æ ¹æ®å®é™…ç´¢å¼•è°ƒæ•´

    # é˜²æ­¢é™¤0
    completed_count[completed_count == 0] = 1
    completed_avg = completed_sum / completed_count
    # æ›¿æ¢orig_dfä¸­modalitieséƒ¨åˆ†
    orig_df.loc[:, dataset_modalities] = completed_avg[:num_rows, :]
    orig_df.to_csv(save_path, index=False)
    print(f"æ»‘åŠ¨çª—å£è¡¥å…¨ç»“æœå·²ä¿å­˜åˆ°: {save_path}")
# src/train.py

"""
train.py

Module to train the T-GAT-UNet model on sliding-window time series dataset with self-supervised masking.
"""


def parse_args():
    parser = argparse.ArgumentParser(description="Train T-GAT-UNet on time series data")
    parser.add_argument("--config", type=str, required=True, help="Path to YAML config file")
    args = parser.parse_args()
    return args


import random
def mask_channel(x, mask_indices, mask_ratio=0.2):
    """
    æœ‰ç›®çš„æ€§é®æ©ï¼šåªé®æ©å®é™…éœ€è¦è¡¥å…¨çš„æ¨¡æ€ï¼ˆmask_indicesæŒ‡å®šï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬éšæœºé®æ©ä¸€ä¸ªéœ€è¡¥å…¨é€šé“ã€‚
    Args:
        x: Tensor of shape [batch, channels, time]
        mask_indices: list of channel indiceséœ€è¦è¢«é®æ©çš„ï¼ˆå³å®é™…ç¼ºå¤±çš„ï¼‰æ¨¡æ€
    Returns:
        x_masked, mask_idx
    """
    batch, C, T = x.size()
    # é®æ©æ‰€æœ‰ have é€šé“ï¼ˆå³ mask_indices æŒ‡å®šçš„æ‰€æœ‰é€šé“ï¼‰ï¼Œç”¨ common_modalities é¢„æµ‹ have é€šé“
    if len(mask_indices) == 0:
        # æ²¡æœ‰éœ€è¦é®æ©çš„é€šé“ï¼Œç›´æ¥è¿”å›åŸæ•°æ®
        return x, torch.tensor([-1]*batch)
    x_masked = x.clone()
    for i in range(batch):
        for idx in mask_indices:
            x_masked[i, idx, :] = 0
    # è¿”å›æ‰€æœ‰è¢«é®æ©çš„é€šé“ç´¢å¼•
    return x_masked, torch.tensor(mask_indices)



# =========================
# åˆ†é˜¶æ®µè®­ç»ƒä¸»å¾ªç¯
# =========================
def train_phased(
    model, dataloader, optimizer, criterion, device,
    mask_indices, phase="encode", classifier=None, gen_optimizer=None, disc_optimizer=None,
    dynamic_need_indices=None
):
    """
    phase: "train"
    - train: è®­ç»ƒç¼–ç å™¨+è§£ç å™¨+åˆ†ç±»å™¨ï¼ˆç«¯åˆ°ç«¯ï¼‰
    """
    model.train()
    ce_loss = torch.nn.CrossEntropyLoss()
    total_loss = 0.0
    total_cls_loss = 0.0
    total_recon_loss = 0.0
    total_correct = 0
    total_samples = 0
    for batch, labels, mask_idx, is_real_mask in tqdm(dataloader, desc=phase.capitalize()):
        batch = batch.to(device)
        labels = labels.to(device)
        is_real_mask = is_real_mask.to(device)
        masked, mask_idx = mask_channel(batch, mask_indices)
        batch_size, C, T = batch.size()
        # === Debug: æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰ NaN/Inf ===
        if torch.isnan(batch).any() or torch.isinf(batch).any():
            print("[DEBUG] Input batch contains NaN or Inf!")
        if torch.isnan(masked).any() or torch.isinf(masked).any():
            print("[DEBUG] Masked input contains NaN or Inf!")
        loss = 0.0
        cls_loss = 0.0
        recon_loss = 0.0
        optimizer.zero_grad()
        for i in range(batch_size):
            window = masked[i].t()  # [T, C]
            out, logits = model(window)  # out: [C, T], logits: [num_classes]            # === Debug: æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦æœ‰ NaN/Inf ===
            if torch.isnan(out).any() or torch.isinf(out).any():
                print(f"[DEBUG] Model output contains NaN or Inf at sample {i}!")
            
            # åªå¯¹çœŸå®é€šé“è®¡ç®—é‡å»ºæŸå¤±
            if is_real_mask.dim() == 2:
                real_channels = is_real_mask[i]
            else:
                real_channels = is_real_mask
            recon_loss_i = 0.0
            real_count = 0
            
            # è·å–commonæ¨¡æ€ç´¢å¼•
            common_indices = getattr(criterion, 'common_indices', [])
            
            for c in range(C):
                target = batch[i, c, :]
                pred = out[c, :]
                
                # åˆ¤æ–­æ˜¯å¦ä¸ºcommonæ¨¡æ€
                is_common_channel = c in common_indices
                
                if is_common_channel:
                    # Commonæ¨¡æ€ï¼šå§‹ç»ˆè®¡ç®—æŸå¤±
                    recon_loss_i = recon_loss_i + criterion(pred, target, channel_idx=c, is_common=True)
                    real_count += 1
                elif real_channels[c]:
                    # Haveæ¨¡æ€ï¼šåªå¯¹çœŸå®é€šé“è®¡ç®—æŸå¤±
                    recon_loss_i = recon_loss_i + criterion(pred, target, channel_idx=c, is_common=False)
                    real_count += 1
            if real_count > 0:
                recon_loss_i = recon_loss_i / real_count
            cls_loss_i = ce_loss(logits.unsqueeze(0), labels[i].unsqueeze(0))
            loss += recon_loss_i + cls_loss_i
            recon_loss += recon_loss_i
            cls_loss += cls_loss_i
            pred_class = logits.argmax(-1).item()
            if pred_class == labels[i].item():
                total_correct += 1
            total_samples += 1
        loss = loss / batch_size
        # === Debug: æ£€æŸ¥ loss æ˜¯å¦æœ‰ NaN/Inf ===
        # é¿å…é‡å¤åŒ…è£¹ tensorï¼Œç›´æ¥ç”¨ float/torch API æ£€æŸ¥
        if isinstance(loss, torch.Tensor):
            if torch.isnan(loss).any() or torch.isinf(loss).any():
                print("[DEBUG] Loss is NaN or Inf after batch!")
        else:
            import math
            if math.isnan(loss) or math.isinf(loss):
                print("[DEBUG] Loss is NaN or Inf after batch!")
        loss.backward()
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        total_loss += loss.item() * batch_size
        total_recon_loss += recon_loss
        total_cls_loss += cls_loss
    n = len(dataloader.dataset)
    acc = total_correct / total_samples if total_samples > 0 else 0.0
    return total_loss / n, total_recon_loss / n, total_cls_loss / n, acc


def eval_loop(model, dataloader, criterion, device, mask_indices):
    model.eval()
    total_loss = 0.0
    total_recon_loss = 0.0
    with torch.no_grad():
        for batch, _, _, is_real_mask in tqdm(dataloader, desc="Eval"):
            batch = batch.to(device)
            is_real_mask = is_real_mask.to(device)
            masked, mask_idx = mask_channel(batch, mask_indices)
            batch_size, C, T = batch.size()
            loss = 0.0
            recon_loss = 0.0
            for i in range(batch_size):
                window = masked[i].t()
                out, _ = model(window)                # åªå¯¹çœŸå®é€šé“è®¡ç®—é‡å»ºæŸå¤±
                real_channels = is_real_mask
                recon_loss_i = 0.0
                real_count = 0
                
                # è·å–commonæ¨¡æ€ç´¢å¼•
                common_indices = getattr(criterion, 'common_indices', [])
                
                for c in range(C):
                    target = batch[i, c, :]
                    pred = out[c, :]
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºcommonæ¨¡æ€
                    is_common_channel = c in common_indices
                    
                    if is_common_channel:
                        # Commonæ¨¡æ€ï¼šå§‹ç»ˆè®¡ç®—æŸå¤±
                        recon_loss_i = recon_loss_i + criterion(pred, target, channel_idx=c, is_common=True)
                        real_count += 1
                    elif real_channels[c]:
                        # Haveæ¨¡æ€ï¼šåªå¯¹çœŸå®é€šé“è®¡ç®—æŸå¤±
                        recon_loss_i = recon_loss_i + criterion(pred, target, channel_idx=c, is_common=False)
                        real_count += 1
                if real_count > 0:
                    recon_loss_i = recon_loss_i / real_count
                loss += recon_loss_i
                recon_loss += recon_loss_i
            loss = loss / batch_size
            total_loss += loss.item() * batch_size
            total_recon_loss += recon_loss
    n = len(dataloader.dataset)
    return total_loss / n, total_recon_loss / n, 0.0, 0.0


# =========================
# æ–°ä¸»å¾ªç¯å…¥å£
# =========================
def main():
    set_seed(42)
    # è®¾ç½® logging ä½¿ INFO çº§åˆ«è¾“å‡ºåˆ°æ§åˆ¶å°
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    args = parse_args()
    # Load config.yaml
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # å‚æ•°
    batch_size = config.get('batch_size', 32)
    epochs = config.get('epochs', 100)
    lr = config.get('lr', 1e-3)
    early_stop_patience = config.get('patience', 10)
    log_dir = config.get('log_dir', 'Logs')
    ckpt_dir = config.get('ckpt_dir', 'Checkpoints')
    mode = config.get('mode', 'train').lower()
    model_path = config.get('model_path', os.path.join(ckpt_dir, 'best_model.pth'))
    common_modalities = config.get('common_modalities', [])
    dataset_modalities_cfg = config.get('dataset_modalities', {})
    # ====== å¤šæ•°æ®é›†è”åˆåŠ è½½ ======
    # åŠ è½½æ‰€æœ‰ data_filesï¼Œæ¨æ–­æ¯ä¸ªæ–‡ä»¶çš„ source datasetï¼Œå¹¶ä¸ºæ¯ä¸ªæ ·æœ¬ä¿ç•™å…¶ need/have ä¿¡æ¯
    from data import load_data, SlidingWindowDataset
    data_files = config.get('data_files', [])
    data_dir = config.get('data_dir', '')
    # æ‹¼æ¥è·¯å¾„
    data_files = [os.path.join(data_dir, f) if not os.path.isabs(f) and not os.path.exists(f) else f for f in data_files]
    all_dfs = []
    all_sources = []
    for f in data_files:
        df = pd.read_csv(f)
        df.columns = [c.strip().lower() for c in df.columns]
        # æ¨æ–­source
        fname = os.path.basename(f).lower()
        if 'fm' in fname:
            source = 'FM'
        elif 'od' in fname:
            source = 'OD'
        elif 'mefar' in fname:
            source = 'MEFAR'
        else:
            source = 'UNKNOWN'
        df['__source__'] = source
        all_dfs.append(df)
        all_sources.append(source)
    data = pd.concat(all_dfs, ignore_index=True)

    # ç»Ÿä¸€modalities
    common_modalities = config.get('common_modalities', [])
    dataset_modalities_cfg = config.get('dataset_modalities', {})
    all_have = []
    all_need = []
    for dsname, modcfg in dataset_modalities_cfg.items():
        all_have.extend(modcfg.get('have', []))
        all_need.extend(modcfg.get('need', []))
    all_modalities = list(dict.fromkeys([m.strip().lower() for m in (common_modalities + all_have + all_need)]))
    block_col = config['block_col'].strip().lower()
    label_col = config.get('label_col', 'F').strip().lower()
    for col in all_modalities:
        if col not in data.columns:
            data[col] = 0.0
    feature_cols = [col for col in all_modalities if col != label_col]
    cols_to_keep = [block_col, label_col] + feature_cols + ['__source__']
    seen2 = set()
    cols_to_keep = [x for x in cols_to_keep if not (x in seen2 or seen2.add(x))]
    data = data[[col for col in cols_to_keep if col in data.columns]]
    data = data.reset_index(drop=True)  # Ensure 0-based integer index

    # Set model input/output/channel parameters after feature_cols is defined
    # (feature_cols is defined after data loading below)

    # Set model input/output/channel parameters after feature_cols is defined
    in_channels = config.get('in_channels', len(feature_cols))
    hidden_channels = config.get('hidden_channels', 64)
    out_channels = config.get('out_channels', in_channels)
    num_classes = config.get('num_classes', 2)

    # ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…need_indicesï¼ˆæ¯ä¸ªsourceä¸åŒï¼‰
    need_indices_list = []
    for i, row in data.iterrows():
        source = row['__source__']
        have_list = common_modalities + dataset_modalities_cfg.get(source, {}).get('have', [])
        need_list = dataset_modalities_cfg.get(source, {}).get('need', [])
        if need_list is None:
            need_list = []
        # need_indices: åœ¨feature_colsä¸­çš„ç´¢å¼•
        need_indices = [feature_cols.index(m) for m in need_list if m in feature_cols]
        if need_indices is None:
            need_indices = []
        need_indices_list.append(need_indices)
    # SlidingWindowDataset expects a DataFrame and global need_indices, so we subclass to support per-sample need_indices
    class MultiDatasetSlidingWindow(SlidingWindowDataset):
        def __init__(self, *args, need_indices_list=None, **kwargs):
            super().__init__(*args, **kwargs)
            self.need_indices_list = need_indices_list
        def __getitem__(self, idx):
            b_idx, start, seg_label = self.indices[idx]
            block = self.blocks[b_idx]
            window_df = block.iloc[start:start + self.window_size]
            # è‡ªåŠ¨å¡«å……ç¼ºå¤±å€¼
            window_df = window_df.fillna(0)
            data_array = window_df[self.feature_cols].values[::self.sampling_rate]
            # æ–­è¨€åŸå§‹æ•°æ®æ—  NaN/Inf
            assert not (np.isnan(data_array).any() or np.isinf(data_array).any()), f"[æ–­è¨€å¤±è´¥] åŸå§‹data_arrayå­˜åœ¨NaN/Inf! idx={idx}"
            # Normalize
            if self.normalize == 'zscore':
                mean = data_array.mean(axis=0, keepdims=True)
                std = data_array.std(axis=0, keepdims=True)
                data_array = (data_array - mean) / (std + 1e-6)
            elif self.normalize == 'minmax':
                min_v = data_array.min(axis=0, keepdims=True)
                max_v = data_array.max(axis=0, keepdims=True)
                data_array = (data_array - min_v) / (max_v - min_v + 1e-6)
            # æ–­è¨€å½’ä¸€åŒ–åæ—  NaN/Inf
            assert not (np.isnan(data_array).any() or np.isinf(data_array).any()), f"[æ–­è¨€å¤±è´¥] å½’ä¸€åŒ–ådata_arrayå­˜åœ¨NaN/Inf! idx={idx}"
            tensor = torch.from_numpy(data_array.T).float()  # [C, T]
            label = int(seg_label)
            label = torch.as_tensor(label, dtype=torch.long)
            # é€šé“å¯ä¿¡mask
            is_real_mask = torch.tensor(self.get_is_real_mask(), dtype=torch.bool)
            # åˆ†é˜¶æ®µé®æ©/è¡¥å…¨é€»è¾‘
            idx_in_data = block.index[start]
            if idx_in_data >= len(self.need_indices_list) or idx_in_data < 0:
                need_indices = []
            else:
                need_indices = self.need_indices_list[idx_in_data]
            if need_indices is None:
                need_indices = []
            if self.phase == "encode":
                if len(need_indices) == 0:
                    mask_idx = -1
                    tensor_masked = tensor
                else:
                    mask_idx = np.random.choice(need_indices)
                    tensor_masked = tensor.clone()
                    tensor_masked[mask_idx, :] = 0
                return tensor_masked, label, mask_idx, is_real_mask
            elif self.phase == "decode":
                if self.dynamic_need and len(need_indices) > 0:
                    need_idx = np.random.choice(need_indices)
                elif len(need_indices) > 0:
                    need_idx = need_indices[0]
                else:
                    need_idx = -1
                tensor_masked = tensor.clone()
                if need_idx != -1:
                    tensor_masked[need_idx, :] = 0
                return tensor_masked, label, need_idx, is_real_mask
            else:
                return tensor, label, -1, is_real_mask

    dataset = MultiDatasetSlidingWindow(
        data=data,
        block_col=block_col,
        feature_cols=feature_cols,
        window_size=config['window_size'],
        step_size=config['step_size'],
        sampling_rate=config.get('sampling_rate', 1),
        normalize=config.get('norm_method'),
        label_col=label_col,
        phase="encode",
        need_indices=[],  # not used, per-sample instead
        dynamic_need=False,
        need_indices_list=need_indices_list
    )
    total = len(dataset)
    train_split = config.get('train_split', 0.6)
    val_split = config.get('val_split', 0.2)
    train_len = round(train_split * total)
    val_len = round(val_split * total)
    test_len = total - train_len - val_len
    if test_len < 0:
        test_len = 0
    while train_len + val_len + test_len < total:
        test_len += 1
    while train_len + val_len + test_len > total:
        if test_len > 0:
            test_len -= 1
        elif val_len > 0:
            val_len -= 1
        else:
            train_len -= 1
    assert train_len + val_len + test_len == total
    print(f"[æ•°æ®é›†åˆ†é…] (å¤šæ•°æ®é›†èåˆ) æ€»æ•°: {total}, è®­ç»ƒ: {train_len}, éªŒè¯: {val_len}, æµ‹è¯•: {test_len}")
    train_ds, val_ds, test_ds = random_split(dataset, [train_len, val_len, test_len])
    
    # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    use_memory_optimizer = config.get('use_memory_optimizer', True)
    if use_memory_optimizer:
        print("ğŸš€ ä½¿ç”¨å†…å­˜ä¼˜åŒ–æ•°æ®åŠ è½½å™¨")
        train_loader = create_memory_efficient_loader(train_ds, config)
        val_loader = create_memory_efficient_loader(val_ds, config)  
        test_loader = create_memory_efficient_loader(test_ds, config)
    else:
        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_ds, batch_size=batch_size)
        test_loader = DataLoader(test_ds, batch_size=batch_size)

    # è¾“å‡ºè®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ
    check_label_distribution(train_ds)

    dataset_modalities = feature_cols
    # é®æ©é€»è¾‘ä¿®æ­£ï¼šé®æ©æ‰€æœ‰ have é€šé“ï¼Œç”¨ common_modalities é¢„æµ‹ have é€šé“ï¼ˆä»¥FM/OD/MEFARå…¨é›†ä¸ºå‡†ï¼‰
    have_list_only = all_have
    mask_indices = [i for i, m in enumerate(dataset_modalities) if m in have_list_only]
    if len(mask_indices) == 0:
        logging.warning("å½“å‰æ•°æ®é›†æ—  have é€šé“å¯ç”¨äºé®æ©ï¼Œè®­ç»ƒæ—¶ä¸ä¼šé®æ©ä»»ä½•é€šé“ã€‚")    # ========== æ¨¡å‹ç»“æ„é€‚é… ==========
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # å†…å­˜ç›‘æ§å™¨
    memory_monitor = MemoryMonitor()
    memory_monitor.check_and_warn("æ¨¡å‹åˆå§‹åŒ–å‰")
    
    # é€‰æ‹©æ¨¡å‹ç‰ˆæœ¬
    use_optimized_model = config.get('use_optimized_model', True)
    use_unet_architecture = config.get('use_unet_architecture', False)
    use_curriculum_learning = config.get('use_curriculum_learning', False)
    enable_attention_visualization = config.get('enable_attention_viz', False)
    
    if use_unet_architecture:
        print("ğŸ—ï¸ ä½¿ç”¨çœŸæ­£çš„U-Netæ¶æ„ UNetTGAT")
        model = UNetTGAT(
            in_channels=in_channels,
            hidden_channels=hidden_channels,            out_channels=out_channels,
            encoder_layers=config.get('encoder_layers', 4),
            heads=config.get('attention_heads', 4),
            time_k=config.get('time_k', 1),
            trans_layers=config.get('transformer_layers', 2),
            num_classes=num_classes
        )
    elif use_optimized_model:
        print("ğŸš€ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæ¨¡å‹ OptimizedTGATUNet")
        model = create_optimized_model(config)
    else:
        print("ä½¿ç”¨æ ‡å‡†ç‰ˆæ¨¡å‹ TGATUNet")
        model = TGATUNet(in_channels, hidden_channels=hidden_channels, out_channels=out_channels, num_classes=num_classes)
    
    model.to(device)
    memory_monitor.check_and_warn("æ¨¡å‹åŠ è½½å")
    optimizer = Adam(model.parameters(), lr=lr)
    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True)
      # åˆ›å»ºæŸå¤±å‡½æ•°
    if config.get('loss_config', {}).get('type') == 'multimodal':
        criterion = create_simple_multimodal_criterion(config)
        print("ä½¿ç”¨å¤šæ¨¡æ€æŸå¤±å‡½æ•°")
    else:
        criterion = MSELoss()
        print("ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°")

    best_val_loss = float('inf')
    epochs_no_improve = 0
    import glob
    latest_model = None
    if os.path.exists(model_path):
        latest_model = model_path
    else:
        model_files = sorted(glob.glob(os.path.join(ckpt_dir, 'best_model.pth')), reverse=True)
        if model_files:
            latest_model = model_files[0]


    # ========== TensorBoard æ—¥å¿—å™¨åˆå§‹åŒ– ========== 
    log_dir_full = os.path.join(log_dir, datetime.now().strftime('%Y%m%d_%H%M%S'))
    os.makedirs(log_dir_full, exist_ok=True)
    
    # åˆ›å»ºå¢å¼ºéªŒè¯ç®¡ç†å™¨
    enhanced_val_manager = EnhancedValidationManager(
        patience=early_stop_patience,
        min_delta=config.get('min_delta', 1e-6),
        val_freq_schedule=config.get('val_freq_schedule'),
        save_dir=os.path.join(log_dir_full, 'enhanced_validation')
    )
    print(f"ğŸ“Š å¯ç”¨å¢å¼ºéªŒè¯ç­–ç•¥ï¼Œè€å¿ƒåº¦={early_stop_patience}")
    writer = SummaryWriter(log_dir=log_dir_full)    # ========== è®­ç»ƒæ¨¡å¼ï¼šæ ‡å‡† vs è¯¾ç¨‹å­¦ä¹  ========== 
    if mode == 'train':
        if use_curriculum_learning:
            print("ğŸ“š ä½¿ç”¨è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ")
            curriculum_trainer = create_curriculum_trainer(model, dataset, config)
            
            for epoch in range(1, epochs + 1):
                # è¯¾ç¨‹å­¦ä¹ è®­ç»ƒ
                train_info = curriculum_trainer.train_epoch(optimizer, criterion, device)
                
                # éªŒè¯
                val_loss, val_recon, _, _ = eval_loop(model, val_loader, criterion, device, mask_indices)
                
                scheduler.step(val_loss)
                current_lr = optimizer.param_groups[0]['lr']
                
                logging.info(f"[CURRICULUM] Epoch {epoch}: "
                           f"train_loss={train_info['loss']:.6f}, "
                           f"val_loss={val_loss:.6f}, "
                           f"difficulty={train_info['difficulty']:.3f}, "
                           f"subset_size={train_info['subset_size']}, "
                           f"lr={current_lr:.6e}")
                
                # TensorBoardè®°å½•
                writer.add_scalar('Curriculum/Difficulty', train_info['difficulty'], epoch)
                writer.add_scalar('Curriculum/SubsetSize', train_info['subset_size'], epoch)
                writer.add_scalar('Curriculum/Performance', train_info['performance'], epoch)
                writer.add_scalar('Train/Loss/train', train_info['loss'], epoch)
                writer.add_scalar('Train/Loss/val', val_loss, epoch)
                writer.add_scalar('Train/LR', current_lr, epoch)
                
                # æ³¨æ„åŠ›å¯è§†åŒ–ï¼ˆæ¯10ä¸ªepochï¼‰
                if enable_attention_visualization and epoch % 10 == 0:
                    print(f"ğŸ” ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ– (Epoch {epoch})...")
                    try:
                        sample_input = next(iter(val_loader))[0][0:1]  # å–ä¸€ä¸ªæ ·æœ¬
                        attention_info = analyze_model_attention(
                            model, sample_input.to(device), 
                            save_dir=f"attention_analysis_epoch_{epoch}"
                        )
                        writer.add_text('Attention/Analysis', str(attention_info), epoch)
                    except Exception as e:
                        print(f"âš ï¸ æ³¨æ„åŠ›å¯è§†åŒ–å¤±è´¥: {e}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    torch.save(model.state_dict(), os.path.join(ckpt_dir, 'best_model.pth'))
                    torch.save(model.state_dict(), model_path)
                    logging.info(f"Saved best model at epoch {epoch}")
                    epochs_no_improve = 0
                else:
                    epochs_no_improve += 1
                
                if epochs_no_improve >= early_stop_patience:
                    logging.info("Early stopping triggered.")
                    break
              # ç»˜åˆ¶è¯¾ç¨‹å­¦ä¹ è¿›åº¦
            curriculum_trainer.plot_curriculum_progress("curriculum_learning_progress.png")
        
        else:
            # æ ‡å‡†è®­ç»ƒæ¨¡å¼
            print("ğŸ¯ ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼")
            for epoch in range(1, epochs + 1):
                train_loss, train_recon, train_cls, train_acc = train_phased(
                    model, train_loader, optimizer, criterion, device, mask_indices
                )
                
                # ä½¿ç”¨å¢å¼ºéªŒè¯ç­–ç•¥
                if enhanced_val_manager.should_validate(epoch):
                    # è®¡ç®—å¢å¼ºéªŒè¯æŒ‡æ ‡
                    val_metrics = enhanced_val_manager.compute_enhanced_validation_metrics(
                        model, val_loader, criterion, device, mask_indices
                    )
                    
                    # æ›´æ–°æŒ‡æ ‡å¹¶è·å–æ—©åœå»ºè®®
                    early_stop_info = enhanced_val_manager.update_metrics(val_metrics, epoch)
                    
                    # è®°å½•ç»“æœ
                    enhanced_val_manager.log_validation_results(val_metrics, epoch, early_stop_info)
                    
                    # å­¦ä¹ ç‡è°ƒåº¦
                    scheduler.step(val_metrics['val_loss'])
                    
                    # è·å–å½“å‰å­¦ä¹ ç‡
                    current_lr = optimizer.param_groups[0]['lr']
                    
                    # å¢å¼ºæ—¥å¿—è®°å½•
                    logging.info(f"[ENHANCED_TRAIN] Epoch {epoch}: "
                               f"train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, "
                               f"val_loss={val_metrics['val_loss']:.6f}, val_acc={val_metrics['val_accuracy']:.4f}, "
                               f"val_f1={val_metrics['val_f1_score']:.4f}, "
                               f"common_recon={val_metrics['val_common_recon_loss']:.6f}, "
                               f"have_recon={val_metrics['val_have_recon_loss']:.6f}, "
                               f"lr={current_lr:.6e}")
                    
                    # TensorBoardè®°å½•å¢å¼ºæŒ‡æ ‡
                    writer.add_scalar('Train/Loss/train', train_loss, epoch)
                    writer.add_scalar('Train/Recon/train', train_recon, epoch)
                    writer.add_scalar('Train/Cls/train', train_cls, epoch)
                    writer.add_scalar('Train/Acc/train', train_acc, epoch)
                    writer.add_scalar('Enhanced_Val/Loss', val_metrics['val_loss'], epoch)
                    writer.add_scalar('Enhanced_Val/Accuracy', val_metrics['val_accuracy'], epoch)
                    writer.add_scalar('Enhanced_Val/F1_Score', val_metrics['val_f1_score'], epoch)
                    writer.add_scalar('Enhanced_Val/Precision', val_metrics['val_precision'], epoch)
                    writer.add_scalar('Enhanced_Val/Recall', val_metrics['val_recall'], epoch)
                    writer.add_scalar('Enhanced_Val/Common_Recon', val_metrics['val_common_recon_loss'], epoch)
                    writer.add_scalar('Enhanced_Val/Have_Recon', val_metrics['val_have_recon_loss'], epoch)
                    writer.add_scalar('Train/LR', current_lr, epoch)
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if epoch == enhanced_val_manager.best_epoch:
                        torch.save(model.state_dict(), os.path.join(ckpt_dir, 'best_model.pth'))
                        torch.save(model.state_dict(), model_path)
                        logging.info(f"Saved best model at epoch {epoch} (composite score: {early_stop_info['best_composite_score']:.6f})")
                        epochs_no_improve = 0
                    else:
                        epochs_no_improve = early_stop_info['epochs_no_improve']
                    
                    # æ—©åœæ£€æŸ¥
                    if early_stop_info['should_stop']:
                        logging.info(f"Enhanced early stopping triggered at epoch {epoch}")
                        break
                        
                    # è¿‡æ‹Ÿåˆè­¦å‘Š
                    if early_stop_info['is_overfitting']:
                        logging.warning(f"[OVERFITTING_DETECTED] Epoch {epoch}: æ£€æµ‹åˆ°å¯èƒ½çš„è¿‡æ‹Ÿåˆ")
                
                else:
                    # ä¸éœ€è¦éªŒè¯çš„epochï¼Œåªåšç®€å•çš„æ—¥å¿—è®°å½•
                    current_lr = optimizer.param_groups[0]['lr']
                    logging.info(f"[TRAIN] Epoch {epoch}: train_loss={train_loss:.6f}, train_acc={train_acc:.4f}, lr={current_lr:.6e}")
                    writer.add_scalar('Train/Loss/train', train_loss, epoch)
                    writer.add_scalar('Train/Acc/train', train_acc, epoch)
                    writer.add_scalar('Train/LR', current_lr, epoch)                
                # === åŠ¨æ€needè¡¥å…¨ ===
                # æ³¨æ„ï¼šä¸ºé¿å…æ•°æ®æ³„éœ²ï¼Œåªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹å¯¹è®­ç»ƒé›†è¿›è¡ŒåŠ¨æ€æ›´æ–°
                if hasattr(train_ds, 'dataset') and hasattr(train_ds.dataset, 'update_need'):
                    # å¦‚æœæ˜¯Subset/RandomSplitåŒ…è£…ï¼Œå–åŸå§‹dataset
                    complete_need_with_model(model, train_ds.dataset, device)
                elif hasattr(train_ds, 'update_need'):
                    complete_need_with_model(model, train_ds, device)
            
            # è®­ç»ƒç»“æŸåä¿å­˜å¢å¼ºéªŒè¯å¯è§†åŒ–
            enhanced_val_manager.save_validation_plots()
            best_summary = enhanced_val_manager.get_best_metrics_summary()
            logging.info(f"Training completed. Best metrics: {best_summary}")
        latest_model = model_path if os.path.exists(model_path) else os.path.join(ckpt_dir, 'best_model.pth')
        # è‡ªåŠ¨è¯„ä¼°
        if latest_model and os.path.exists(latest_model):
            model.load_state_dict(torch.load(latest_model))
            test_loss, test_recon, _, _ = train_phased(
                model, test_loader, optimizer, criterion, device, mask_indices
            )
            logging.info(f"[Train End] Test Loss: {test_loss:.6f}, Recon Loss: {test_recon:.6f}")
            writer.add_scalar('Loss/test', test_loss)
            writer.add_scalar('Recon/test', test_recon)
        else:
            logging.warning(f"æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•è¯„ä¼°ï¼(æŸ¥æ‰¾è·¯å¾„: {latest_model})")
    elif mode == 'eval':
        if latest_model and os.path.exists(latest_model):
            model.load_state_dict(torch.load(latest_model))
            logging.info(f"[Eval Mode] åŠ è½½æ¨¡å‹: {latest_model}")
            test_loss, test_recon, _, _ = train_phased(
                model, test_loader, optimizer, criterion, device, mask_indices, phase="decode"
            )
            logging.info(f"[Eval Mode] Test Loss: {test_loss:.6f}, Recon Loss: {test_recon:.6f}")
            writer.add_scalar('Loss/test', test_loss)
            writer.add_scalar('Recon/test', test_recon)
            # ========== è¯„ä¼°æ¨¡å¼ä¸‹ä¸å†åšæ»‘åŠ¨çª—å£è¡¥å…¨å’Œä¿å­˜csv ========== 
        else:
            logging.warning(f"æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•è¯„ä¼°ï¼(æŸ¥æ‰¾è·¯å¾„: {latest_model})")
    else:
        logging.error(f"æœªçŸ¥æ¨¡å¼: {mode}ï¼Œè¯·åœ¨config.yamlä¸­è®¾ç½® mode: train æˆ– mode: eval")
    writer.close()

if __name__ == '__main__':
    main()

