# memory_optimizer.py - å†…å­˜ä¼˜åŒ–æ¨¡å—

import torch
import numpy as np
from torch.utils.data import DataLoader
import gc
from typing import Iterator, Tuple
import psutil
import warnings

class MemoryEfficientDataLoader:
    """å†…å­˜é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, dataset, batch_size=32, shuffle=True, 
                 memory_threshold=0.8, pin_memory=False):
        self.dataset = dataset
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.memory_threshold = memory_threshold
        self.pin_memory = pin_memory and torch.cuda.is_available()
        
    def _check_memory_usage(self):
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡"""
        memory_percent = psutil.virtual_memory().percent / 100.0
        return memory_percent
    
    def _adaptive_batch_size(self):
        """æ ¹æ®å†…å­˜ä½¿ç”¨æƒ…å†µè‡ªé€‚åº”è°ƒæ•´æ‰¹å¤§å°"""
        memory_usage = self._check_memory_usage()
        if memory_usage > self.memory_threshold:
            new_batch_size = max(1, self.batch_size // 2)
            warnings.warn(f"å†…å­˜ä½¿ç”¨ç‡ {memory_usage:.1%} è¿‡é«˜ï¼Œæ‰¹å¤§å°è°ƒæ•´ä¸º {new_batch_size}")
            return new_batch_size
        return self.batch_size
    
    def __iter__(self) -> Iterator[Tuple[torch.Tensor, ...]]:
        """å†…å­˜ä¼˜åŒ–çš„è¿­ä»£å™¨"""
        indices = list(range(len(self.dataset)))
        if self.shuffle:
            np.random.shuffle(indices)
        
        current_batch_size = self._adaptive_batch_size()
        
        for i in range(0, len(indices), current_batch_size):
            # æ£€æŸ¥å†…å­˜å¹¶å¯èƒ½è°ƒæ•´æ‰¹å¤§å°
            if i % (current_batch_size * 10) == 0:
                current_batch_size = self._adaptive_batch_size()
            
            batch_indices = indices[i:i + current_batch_size]
            batch_data = []
            
            for idx in batch_indices:
                item = self.dataset[idx]
                batch_data.append(item)
            
            # è½¬æ¢ä¸ºå¼ é‡æ‰¹æ¬¡
            if batch_data:
                batch = self._collate_fn(batch_data)
                yield batch
                
                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                if len(batch_indices) > 16:
                    del batch_data, batch
                    gc.collect()
    
    def _collate_fn(self, batch):
        """è‡ªå®šä¹‰æ‰¹æ¬¡æ•´ç†å‡½æ•°"""
        if len(batch) == 0:
            return None
        
        # å‡è®¾æ¯ä¸ªæ ·æœ¬è¿”å› (x, label, mask_idx, is_real_mask)
        xs, labels, mask_idxs, is_real_masks = zip(*batch)
        
        # å †å å¼ é‡
        x_batch = torch.stack(xs, dim=0)
        label_batch = torch.stack(labels, dim=0) if isinstance(labels[0], torch.Tensor) else torch.tensor(labels)
        
        # å¤„ç†mask_idxï¼ˆå¯èƒ½æ˜¯intæˆ–tensorï¼‰
        if isinstance(mask_idxs[0], torch.Tensor):
            mask_idx_batch = torch.stack(mask_idxs, dim=0)
        else:
            mask_idx_batch = torch.tensor(mask_idxs)
        
        # å¤„ç†is_real_mask
        if isinstance(is_real_masks[0], torch.Tensor):
            is_real_mask_batch = torch.stack(is_real_masks, dim=0)
        else:
            is_real_mask_batch = torch.tensor(is_real_masks)
        
        return x_batch, label_batch, mask_idx_batch, is_real_mask_batch

class MemoryMonitor:
    """å†…å­˜ç›‘æ§å™¨"""
    
    def __init__(self, warning_threshold=0.8, critical_threshold=0.9):
        self.warning_threshold = warning_threshold
        self.critical_threshold = critical_threshold
        
    def check_and_warn(self, context=""):
        """æ£€æŸ¥å†…å­˜å¹¶å‘å‡ºè­¦å‘Š"""
        memory_info = psutil.virtual_memory()
        usage_percent = memory_info.percent / 100.0
        
        if usage_percent > self.critical_threshold:
            print(f"ğŸš¨ ä¸¥é‡è­¦å‘Š {context}: å†…å­˜ä½¿ç”¨ç‡ {usage_percent:.1%} è¿‡é«˜ï¼")
            self._emergency_cleanup()
            return "critical"
        elif usage_percent > self.warning_threshold:
            print(f"âš ï¸ è­¦å‘Š {context}: å†…å­˜ä½¿ç”¨ç‡ {usage_percent:.1%}")
            return "warning"
        return "normal"
    
    def _emergency_cleanup(self):
        """ç´§æ€¥å†…å­˜æ¸…ç†"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def optimize_tensor_memory(tensor, dtype=torch.float16):
    """å¼ é‡å†…å­˜ä¼˜åŒ–"""
    if tensor.dtype == torch.float32 and dtype == torch.float16:
        return tensor.half()
    return tensor

def create_memory_efficient_loader(dataset, config):
    """åˆ›å»ºå†…å­˜é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨"""
    batch_size = config.get('batch_size', 32)
    
    # æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´æ‰¹å¤§å°
    available_memory_gb = psutil.virtual_memory().available / (1024**3)
    if available_memory_gb < 4:
        batch_size = min(batch_size, 8)
        print(f"å¯ç”¨å†…å­˜è¾ƒå°‘({available_memory_gb:.1f}GB)ï¼Œæ‰¹å¤§å°è°ƒæ•´ä¸º {batch_size}")
    elif available_memory_gb < 8:
        batch_size = min(batch_size, 16)
    
    return MemoryEfficientDataLoader(
        dataset, 
        batch_size=batch_size,
        memory_threshold=0.8,
        pin_memory=torch.cuda.is_available()
    )
