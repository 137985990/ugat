# performance_test.py - æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•è„šæœ¬

import time
import torch
import psutil
import numpy as np
from contextlib import contextmanager
from typing import Dict, List
from visualization import chart_generator, safe_plot_with_fallback

class PerformanceProfiler:
    """æ€§èƒ½åˆ†æå™¨"""
    
    def __init__(self):
        self.metrics = {}
        self.start_time = None
        self.start_memory = None
        
    @contextmanager
    def profile(self, name: str):
        """æ€§èƒ½åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_memory = psutil.virtual_memory().used / 1024**3  # GB
        
        if torch.cuda.is_available():
            torch.cuda.synchronize()
            start_gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        else:
            start_gpu_memory = 0
        
        try:
            yield
        finally:
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            end_memory = psutil.virtual_memory().used / 1024**3
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
                end_gpu_memory = torch.cuda.memory_allocated() / 1024**3
            else:
                end_gpu_memory = 0
            
            # è®¡ç®—æŒ‡æ ‡
            duration = end_time - start_time
            memory_delta = end_memory - start_memory
            gpu_memory_delta = end_gpu_memory - start_gpu_memory
            
            self.metrics[name] = {
                'duration': duration,
                'memory_delta': memory_delta,
                'gpu_memory_delta': gpu_memory_delta,
                'peak_memory': end_memory,
                'peak_gpu_memory': end_gpu_memory
            }
            
            print(f"[{name}] è€—æ—¶: {duration:.3f}s, å†…å­˜å¢é‡: {memory_delta:.2f}GB, GPUå†…å­˜å¢é‡: {gpu_memory_delta:.2f}GB")

def benchmark_models():
    """å¯¹æ¯”æ ‡å‡†æ¨¡å‹å’Œä¼˜åŒ–æ¨¡å‹çš„æ€§èƒ½"""
    import yaml
    from model import TGATUNet
    from model_optimized import OptimizedTGATUNet
    
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    batch_size = 8  # å°æ‰¹é‡æµ‹è¯•
    seq_len = 320
    in_channels = 32
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_input = torch.randn(seq_len, in_channels).to(device)
    
    profiler = PerformanceProfiler()
    
    # æµ‹è¯•æ ‡å‡†æ¨¡å‹
    print("ğŸ” æµ‹è¯•æ ‡å‡† TGATUNet æ¨¡å‹...")
    model_std = TGATUNet(
        in_channels=in_channels,
        hidden_channels=64,
        out_channels=in_channels,
        num_classes=2
    ).to(device)
    
    model_std.eval()
    with torch.no_grad():
        with profiler.profile("TGATUNet_inference"):
            for _ in range(10):  # å¤šæ¬¡æ¨ç†å–å¹³å‡
                output, logits = model_std(test_input)
    
    # æµ‹è¯•ä¼˜åŒ–æ¨¡å‹
    print("ğŸš€ æµ‹è¯•ä¼˜åŒ– OptimizedTGATUNet æ¨¡å‹...")
    model_opt = OptimizedTGATUNet(
        in_channels=in_channels,
        hidden_channels=64,
        out_channels=in_channels,
        encoder_layers=2,
        decoder_layers=2,
        heads=2,
        trans_layers=1,
        num_classes=2,
        use_checkpoint=False  # æ¨ç†æ—¶å…³é—­checkpoint
    ).to(device)
    
    model_opt.eval()
    with torch.no_grad():
        with profiler.profile("OptimizedTGATUNet_inference"):
            for _ in range(10):  # å¤šæ¬¡æ¨ç†å–å¹³å‡
                output, logits = model_opt(test_input)
    
    # è®¡ç®—å‚æ•°é‡
    std_params = sum(p.numel() for p in model_std.parameters())
    opt_params = sum(p.numel() for p in model_opt.parameters())
    
    print(f"\nğŸ“Š æ¨¡å‹å¯¹æ¯”ç»“æœ:")
    print(f"æ ‡å‡†æ¨¡å‹å‚æ•°é‡: {std_params:,}")
    print(f"ä¼˜åŒ–æ¨¡å‹å‚æ•°é‡: {opt_params:,}")
    print(f"å‚æ•°å‡å°‘: {(std_params - opt_params) / std_params * 100:.1f}%")
    
    # æ¨ç†æ—¶é—´å¯¹æ¯”
    std_time = profiler.metrics['TGATUNet_inference']['duration'] / 10
    opt_time = profiler.metrics['OptimizedTGATUNet_inference']['duration'] / 10
    print(f"æ ‡å‡†æ¨¡å‹æ¨ç†æ—¶é—´: {std_time:.4f}s")
    print(f"ä¼˜åŒ–æ¨¡å‹æ¨ç†æ—¶é—´: {opt_time:.4f}s")
    print(f"æ¨ç†åŠ é€Ÿ: {std_time / opt_time:.2f}x")
    
    return profiler.metrics

def benchmark_graph_building():
    """å›¾æ„å»ºæ€§èƒ½æµ‹è¯•"""
    from graph import build_graph
    from graph_cache import build_graph_cached
    
    seq_lengths = [64, 128, 256, 320, 512]
    channels = 32
    time_k = 1
    
    profiler = PerformanceProfiler()
    
    results = {'standard': [], 'cached': []}
    
    for seq_len in seq_lengths:
        print(f"\nğŸ“ æµ‹è¯•åºåˆ—é•¿åº¦: {seq_len}")
        test_data = torch.randn(seq_len, channels)
        
        # æµ‹è¯•æ ‡å‡†å›¾æ„å»º
        with profiler.profile(f"standard_graph_{seq_len}"):
            for _ in range(100):  # å¤šæ¬¡æ„å»º
                graph = build_graph(test_data, time_k=time_k)
        
        # æµ‹è¯•ç¼“å­˜å›¾æ„å»º
        with profiler.profile(f"cached_graph_{seq_len}"):
            for _ in range(100):  # å¤šæ¬¡æ„å»º
                graph = build_graph_cached(test_data, time_k=time_k)
        
        std_time = profiler.metrics[f"standard_graph_{seq_len}"]['duration'] / 100
        cached_time = profiler.metrics[f"cached_graph_{seq_len}"]['duration'] / 100
        
        results['standard'].append(std_time)
        results['cached'].append(cached_time)
        print(f"æ ‡å‡†æ„å»º: {std_time:.6f}s, ç¼“å­˜æ„å»º: {cached_time:.6f}s, åŠ é€Ÿ: {std_time/cached_time:.2f}x")
    
    # ä½¿ç”¨æ–°çš„å¯è§†åŒ–æ¨¡å—ç”Ÿæˆå›¾è¡¨
    try:
        chart_generator.plot_performance_comparison(
            seq_lengths=seq_lengths, 
            results=results, 
            save_path='graph_benchmark',
            use_chinese=True
        )
    except Exception as e:
        print(f"âš ï¸ ä¸­æ–‡å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        print("ğŸ”„ ä½¿ç”¨è‹±æ–‡é‡æ–°ç”Ÿæˆ...")
        try:
            chart_generator.plot_performance_comparison(
                seq_lengths=seq_lengths, 
                results=results, 
                save_path='graph_benchmark',
                use_chinese=False
            )
        except Exception as e2:
            print(f"âŒ å›¾è¡¨ç”Ÿæˆå®Œå…¨å¤±è´¥: {e2}")
            print("ğŸ’¡ å°†ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶...")
            # ä¿å­˜åŸå§‹æ•°æ®
            import json
            with open('benchmark_data.json', 'w') as f:
                json.dump({
                    'seq_lengths': seq_lengths,
                    'results': results
                }, f, indent=2)
            print("ğŸ“Š åŸºå‡†æ•°æ®å·²ä¿å­˜åˆ° benchmark_data.json")
    
    return results

def benchmark_memory_usage():
    """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
    from memory_optimizer import MemoryEfficientDataLoader, MemoryMonitor
    from torch.utils.data import TensorDataset, DataLoader
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    size = 1000
    seq_len = 320
    channels = 32
    
    x = torch.randn(size, channels, seq_len)
    y = torch.randint(0, 2, (size,))
    mask_idx = torch.randint(0, channels, (size,))
    is_real = torch.ones(size, channels, dtype=torch.bool)
    
    dataset = TensorDataset(x, y, mask_idx, is_real)
    
    monitor = MemoryMonitor()
    profiler = PerformanceProfiler()
    
    batch_sizes = [8, 16, 32, 64]
    
    for batch_size in batch_sizes:
        print(f"\nğŸ§ª æµ‹è¯•æ‰¹å¤§å°: {batch_size}")
        
        # æ ‡å‡†æ•°æ®åŠ è½½å™¨
        monitor.check_and_warn("æ ‡å‡†åŠ è½½å™¨å‰")
        with profiler.profile(f"standard_loader_{batch_size}"):
            loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
            for i, batch in enumerate(loader):
                if i >= 10:  # åªæµ‹è¯•å‰10ä¸ªæ‰¹æ¬¡
                    break
        monitor.check_and_warn("æ ‡å‡†åŠ è½½å™¨å")
        
        # ä¼˜åŒ–æ•°æ®åŠ è½½å™¨
        monitor.check_and_warn("ä¼˜åŒ–åŠ è½½å™¨å‰")
        with profiler.profile(f"memory_loader_{batch_size}"):
            loader = MemoryEfficientDataLoader(dataset, batch_size=batch_size)
            for i, batch in enumerate(loader):
                if i >= 10:  # åªæµ‹è¯•å‰10ä¸ªæ‰¹æ¬¡
                    break
        monitor.check_and_warn("ä¼˜åŒ–åŠ è½½å™¨å")
    
    return profiler.metrics

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    print("\n" + "="*50)
    print("1. æ¨¡å‹æ¨ç†æ€§èƒ½æµ‹è¯•")
    print("="*50)
    model_results = benchmark_models()
    
    print("\n" + "="*50)
    print("2. å›¾æ„å»ºæ€§èƒ½æµ‹è¯•")
    print("="*50)
    graph_results = benchmark_graph_building()
    
    print("\n" + "="*50)
    print("3. å†…å­˜ä½¿ç”¨æµ‹è¯•")
    print("="*50)
    memory_results = benchmark_memory_usage()
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    
    # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
    try:
        chart_generator.plot_memory_usage(memory_results, 'memory_analysis')
        chart_generator.plot_model_comparison(model_results, 'model_analysis')
        print("ğŸ“Š æ‰€æœ‰å›¾è¡¨å·²ç”Ÿæˆï¼")
    except Exception as e:
        print(f"âš ï¸ å›¾è¡¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜: {e}")
        print("ğŸ’¡ å»ºè®®æ£€æŸ¥ä¾èµ–åº“å®‰è£…: pip install matplotlib seaborn")
    
    print("ğŸ“„ è¯¦ç»†ç»“æœæ–‡ä»¶:")
    print("  - graph_benchmark.png/.pdf/.svg - å›¾æ„å»ºæ€§èƒ½å¯¹æ¯”")
    print("  - memory_analysis.png/.pdf - å†…å­˜ä½¿ç”¨åˆ†æ") 
    print("  - model_analysis.png/.pdf - æ¨¡å‹æ€§èƒ½å¯¹æ¯”")
    print("  - benchmark_data.json - åŸå§‹æ•°æ®å¤‡ä»½")
