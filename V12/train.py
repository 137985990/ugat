# train.py - V12å®Œæ•´è®­ç»ƒè„šæœ¬ï¼ˆä¼˜åŒ–ç‰ˆï¼šé«˜æ˜¾å­˜åˆ©ç”¨+ä½å†…å­˜å ç”¨ï¼‰

def set_seed(seed=42):
    import random
    import numpy as np
    import torch
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # ä¿è¯éƒ¨åˆ†åº“çš„ç¡®å®šæ€§
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

import collections
import torch
import torch.nn as nn
from torch.optim import Adam
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.nn import MSELoss
from torch.utils.data import DataLoader, random_split
from torch.utils.tensorboard.writer import SummaryWriter
from torch.nn.utils.clip_grad import clip_grad_norm_  # æ­£ç¡®çš„å¯¼å…¥æ–¹å¼

# ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨å’Œå†…å­˜ç›‘æ§
try:
    from optimized_data_loader import OptimizedDataLoader, print_memory_usage
    OPTIMIZED_LOADER_AVAILABLE = True
except ImportError:
    OPTIMIZED_LOADER_AVAILABLE = False
    print("Warning: Optimized data loader not available, using standard loader.")

# æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒæ”¯æŒ
try:
    from torch.cuda.amp.grad_scaler import GradScaler
    from torch.cuda.amp.autocast_mode import autocast
    AMP_AVAILABLE = True
except ImportError:
    try:
        from torch.cuda.amp import GradScaler, autocast
        AMP_AVAILABLE = True
    except ImportError:
        AMP_AVAILABLE = False
import argparse
import yaml
import os
import pandas as pd
from datetime import datetime
import logging
from tqdm import tqdm
import numpy as np

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from simple_multimodal_integration import create_simple_multimodal_criterion
from enhanced_validation_integration import EnhancedValidationManager

def check_label_distribution(dataset):
    """æ£€æŸ¥å¹¶è¾“å‡ºæ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒå’Œæ‰€æœ‰æ ‡ç­¾ç§ç±»"""
    label_counter = collections.Counter()
    all_labels = set()
    for i in range(len(dataset)):
        item = dataset[i]
        label = item[1]
        if hasattr(label, 'item'):
            label = label.item()
        label_counter[label] += 1
        all_labels.add(label)
    print("æ ‡ç­¾åˆ†å¸ƒ:", dict(label_counter))
    print("æ‰€æœ‰æ ‡ç­¾:", sorted(list(all_labels)))
    return label_counter, all_labels

def complete_need_with_model(model, dataset, device):
    """ç”¨æ¨¡å‹å¯¹æ•´ä¸ªæ•°æ®é›†çš„needé€šé“è¿›è¡Œè¡¥å…¨ï¼Œå¹¶å†™å›datasetï¼ˆåŠ¨æ€needæ›´æ–°ï¼‰"""
    model.eval()
    from torch.utils.data import DataLoader
    import torch
    loader = DataLoader(dataset, batch_size=32)
    all_preds = []
    sample_base_idx = 0
    with torch.no_grad():
        for batch in tqdm(loader, desc="Complete need (dynamic)"):
            if len(batch) == 5:
                batch_x, _, _, _, _ = batch
            else:
                batch_x = batch[0]
            
            batch_x = batch_x.to(device)
            batch_size, C, T = batch_x.size()
            for i in range(batch_size):
                window = batch_x[i].t()
                out, _ = model(window)
                out = out.t()
                all_preds.append(out.cpu())
            sample_base_idx += batch_size

    # å†™å›æ•°æ®é›†
    for idx, pred in enumerate(all_preds):
        if hasattr(dataset, 'update_need'):
            dataset.update_need(idx, pred)

def mask_channel(batch, mask_indices):
    """å¯¹batchä¸­çš„æŒ‡å®šé€šé“è¿›è¡Œmask"""
    masked = batch.clone()
    if isinstance(mask_indices, (list, tuple)):
        for idx in mask_indices:
            if idx < batch.size(1):
                masked[:, idx, :] = 0
    else:
        if mask_indices < batch.size(1):
            masked[:, mask_indices, :] = 0
    return masked, mask_indices

def train_phased_with_grad_accumulation(model, dataloader, optimizer, criterion, device, mask_indices, 
                                      accumulate_grad_batches=2, use_mixed_precision=True, scaler=None):
    """è®­ç»ƒå‡½æ•° - æ”¯æŒæ¢¯åº¦ç´¯ç§¯çš„æ‰¹é‡å¤„ç†ç‰ˆæœ¬"""
    model.train()
    kl_div_loss = torch.nn.KLDivLoss(reduction='batchmean')
    
    total_loss = 0.0
    total_recon_loss = 0.0
    total_cls_consistency_loss = 0.0
    total_encode_correct = 0
    total_decode_correct = 0
    total_samples = 0
    
    # è·å–commonæ¨¡æ€ç´¢å¼•
    common_indices = getattr(criterion, 'common_indices', [])
      # æ¢¯åº¦ç´¯ç§¯ç›¸å…³
    accumulated_loss = 0.0
    step_count = 0
    
    for batch_idx, batch_data in enumerate(tqdm(dataloader, desc="Training")):
        if len(batch_data) == 4:
            batch, labels, _, is_real_mask = batch_data
        else:
            batch, labels, _, is_real_mask, _ = batch_data
        
        # ç«‹å³è½¬ç§»åˆ°GPU
        batch = batch.to(device, non_blocking=True)
        labels = labels.to(device, non_blocking=True)
        is_real_mask = is_real_mask.to(device, non_blocking=True)
        
        masked, mask_idx = mask_channel(batch, mask_indices)
        batch_size, C, T = batch.size()        
        # æ¢¯åº¦ç´¯ç§¯ï¼šåªåœ¨ç´¯ç§¯å‘¨æœŸå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
        if step_count % accumulate_grad_batches == 0:
            optimizer.zero_grad()
        
        if use_mixed_precision and scaler is not None and AMP_AVAILABLE:
            with autocast():
                # çœŸæ­£çš„æ‰¹é‡å‰å‘ä¼ æ’­
                batch_out_encode, batch_logits_encode = forward_batch_parallel(model, masked, device)
                
                # ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­ï¼šç”¨é‡å»ºæ•°æ®å†æ¬¡åˆ†ç±»
                # batch_out_encodeæ˜¯[batch_size, C, T]ï¼Œç›´æ¥ä½¿ç”¨æ— éœ€è½¬ç½®
                batch_out_decode, batch_logits_decode = forward_batch_parallel(model, batch_out_encode, device)
                
                # é«˜æ•ˆçš„æ‰¹é‡é‡å»ºæŸå¤±è®¡ç®—
                recon_loss = compute_batch_recon_loss(batch, batch_out_encode, is_real_mask, 
                                                    common_indices, criterion, C, batch_size)
                
                # æ‰¹é‡åˆ†ç±»ä¸€è‡´æ€§æŸå¤±
                log_softmax_decode = torch.nn.functional.log_softmax(batch_logits_decode, dim=1)
                softmax_encode = torch.nn.functional.softmax(batch_logits_encode, dim=1)
                cls_consistency_loss = kl_div_loss(log_softmax_decode, softmax_encode)
                
                loss = (recon_loss + cls_consistency_loss) / accumulate_grad_batches  # æ ‡å‡†åŒ–æ¢¯åº¦
            
            # æ··åˆç²¾åº¦åå‘ä¼ æ’­
            scaler.scale(loss).backward()
            
            # åœ¨ç´¯ç§¯å‘¨æœŸç»“æŸæ—¶æ›´æ–°å‚æ•°
            if (step_count + 1) % accumulate_grad_batches == 0:
                scaler.unscale_(optimizer)
                clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
        else:
            # æ ‡å‡†ç²¾åº¦æ‰¹é‡å¤„ç†
            batch_out_encode, batch_logits_encode = forward_batch_parallel(model, masked, device)
            
            # ç¬¬äºŒæ¬¡å‰å‘ä¼ æ’­ï¼šç”¨é‡å»ºæ•°æ®å†æ¬¡åˆ†ç±»
            # batch_out_encodeæ˜¯[batch_size, C, T]ï¼Œç›´æ¥ä½¿ç”¨æ— éœ€è½¬ç½®
            batch_out_decode, batch_logits_decode = forward_batch_parallel(model, batch_out_encode, device)
            
            # é«˜æ•ˆçš„æ‰¹é‡é‡å»ºæŸå¤±è®¡ç®—
            recon_loss = compute_batch_recon_loss(batch, batch_out_encode, is_real_mask, 
                                                common_indices, criterion, C, batch_size)
            
            # æ‰¹é‡åˆ†ç±»ä¸€è‡´æ€§æŸå¤±
            log_softmax_decode = torch.nn.functional.log_softmax(batch_logits_decode, dim=1)
            softmax_encode = torch.nn.functional.softmax(batch_logits_encode, dim=1)
            cls_consistency_loss = kl_div_loss(log_softmax_decode, softmax_encode)
            
            loss = (recon_loss + cls_consistency_loss) / accumulate_grad_batches  # æ ‡å‡†åŒ–æ¢¯åº¦
            loss.backward()
            
            # åœ¨ç´¯ç§¯å‘¨æœŸç»“æŸæ—¶æ›´æ–°å‚æ•°
            if (step_count + 1) % accumulate_grad_batches == 0:
                clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
        
        # ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨åŸå§‹æŸå¤±å€¼ï¼Œä¸é™¤ä»¥accumulate_grad_batchesï¼‰
        original_loss = loss * accumulate_grad_batches
        original_recon = recon_loss
        original_cls = cls_consistency_loss
        
        # æ‰¹é‡å‡†ç¡®ç‡è®¡ç®—
        pred_encode = torch.argmax(batch_logits_encode, dim=1)
        pred_decode = torch.argmax(batch_logits_decode, dim=1)
        
        total_encode_correct += (pred_encode == labels).sum().item()
        total_decode_correct += (pred_decode == labels).sum().item()
        total_samples += batch_size
          # å®‰å…¨åœ°è·å–æŸå¤±å€¼
        if isinstance(original_loss, torch.Tensor):
            loss_val = original_loss.item()
        else:
            loss_val = float(original_loss)
            
        if isinstance(original_recon, torch.Tensor):
            recon_val = original_recon.item()
        else:
            recon_val = float(original_recon)
            
        if isinstance(original_cls, torch.Tensor):
            cls_val = original_cls.item()
        else:
            cls_val = float(original_cls)
        
        total_loss += loss_val * batch_size
        total_recon_loss += recon_val * batch_size
        total_cls_consistency_loss += cls_val * batch_size
        
        step_count += 1
        
        # æ˜¾å­˜æ¸…ç†
        del batch, labels, is_real_mask, masked, batch_out_encode, batch_logits_encode
        del batch_out_decode, batch_logits_decode, loss, recon_loss, cls_consistency_loss
        torch.cuda.empty_cache()
    
    # å¤„ç†æœ€åçš„ä¸å®Œæ•´ç´¯ç§¯æ‰¹æ¬¡
    if step_count % accumulate_grad_batches != 0:
        if use_mixed_precision and scaler is not None and AMP_AVAILABLE:
            scaler.unscale_(optimizer)
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
        else:
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
    
    n = len(dataloader.dataset)
    encode_acc = total_encode_correct / total_samples if total_samples > 0 else 0.0
    decode_acc = total_decode_correct / total_samples if total_samples > 0 else 0.0
    return total_loss / n, total_recon_loss / n, total_cls_consistency_loss / n, encode_acc, decode_acc


def monitor_gpu_usage(prefix=""):
    """ç›‘æ§GPUä½¿ç”¨æƒ…å†µ - ç®€åŒ–ç‰ˆ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / (1024**3)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        utilization = (allocated / total_memory) * 100
        return utilization
    return 0


def forward_batch_parallel(model, input_batch, device):
    """çœŸæ­£çš„æ‰¹é‡å¹¶è¡Œå‰å‘ä¼ æ’­ï¼Œæœ€å¤§åŒ–åˆ©ç”¨16GBæ˜¾å­˜"""
    batch_size, C, T = input_batch.size()
    
    # æ‰¹é‡è½¬ç½®: [batch_size, C, T] -> [batch_size, T, C] 
    windows = input_batch.transpose(1, 2)  # [batch_size, T, C]
      # å°è¯•çœŸæ­£çš„æ‰¹é‡å¤„ç† - ç›´æ¥å¤„ç†æ•´ä¸ªbatch
    try:
        # æ–¹æ³•1: å¦‚æœæ¨¡å‹æ”¯æŒæ‰¹é‡è¾“å…¥ï¼Œç›´æ¥å¤„ç†
        batch_out, batch_logits = model.forward_batch(windows)
        return batch_out, batch_logits  # [batch_size, C, T], [batch_size, num_classes]
    except AttributeError:
        # æ–¹æ³•2: ä½¿ç”¨torch.vmapè¿›è¡Œå‘é‡åŒ–æ‰¹é‡å¤„ç†
        try:
            def single_forward(window):
                out, logits = model(window)
                return out.t(), logits  # [C, T], [num_classes]
            
            # ä½¿ç”¨vmapè¿›è¡ŒçœŸæ­£çš„å¹¶è¡ŒåŒ–
            vmapped_forward = torch.vmap(single_forward, in_dims=0, out_dims=0)
            batch_out, batch_logits = vmapped_forward(windows)
            return batch_out, batch_logits  # [batch_size, C, T], [batch_size, num_classes]
        except:
            # æ–¹æ³•3: ä¼˜åŒ–çš„å¤§æ‰¹é‡å¤„ç† - å……åˆ†åˆ©ç”¨æ˜¾å­˜
            return forward_large_batch_optimized(model, windows, device)


def forward_large_batch_optimized(model, windows, device):
    """ä¼˜åŒ–çš„å¤§æ‰¹é‡å¤„ç†ï¼Œå……åˆ†åˆ©ç”¨16GBæ˜¾å­˜"""
    batch_size, T, C = windows.size()
    
    # æ ¹æ®æ˜¾å­˜è‡ªé€‚åº”è°ƒæ•´chunkå¤§å° - æ›´å¤§çš„å—
    if torch.cuda.get_device_properties(0).total_memory > 15e9:  # 16GBæ˜¾å­˜
        chunk_size = min(batch_size, 32)  # å¤§å¹…å¢åŠ chunkå¤§å°
    else:
        chunk_size = min(batch_size, 16)
    
    batch_outputs = []
    batch_logits = []
    
    # ä½¿ç”¨æ›´å¤§çš„chunkè¿›è¡Œå¹¶è¡Œå¤„ç†
    for i in range(0, batch_size, chunk_size):
        end_idx = min(i + chunk_size, batch_size)
        chunk_windows = windows[i:end_idx]  # [chunk_size, T, C]
        chunk_size_actual = chunk_windows.size(0)
        
        # æ‰¹é‡å¤„ç†chunk
        chunk_out_list = []
        chunk_logits_list = []
        
        # å¹¶è¡Œå¤„ç†chunkä¸­çš„æ‰€æœ‰æ ·æœ¬
        for j in range(chunk_size_actual):
            out, logits = model(chunk_windows[j])
            chunk_out_list.append(out.t())  # [C, T]
            chunk_logits_list.append(logits)
        
        # æ‰¹é‡å †å chunkç»“æœ
        chunk_out = torch.stack(chunk_out_list, dim=0)  # [chunk_size, C, T]
        chunk_logits = torch.stack(chunk_logits_list, dim=0)  # [chunk_size, num_classes]
        
        batch_outputs.append(chunk_out)
        batch_logits.append(chunk_logits)
    
    # æœ€ç»ˆå †å æ‰€æœ‰chunk
    final_batch_out = torch.cat(batch_outputs, dim=0)  # [batch_size, C, T]
    final_batch_logits = torch.cat(batch_logits, dim=0)  # [batch_size, num_classes]
    
    return final_batch_out, final_batch_logits


def compute_batch_recon_loss(targets, predictions, is_real_mask, common_indices, criterion, C, batch_size):
    """é«˜æ•ˆçš„æ‰¹é‡é‡å»ºæŸå¤±è®¡ç®—"""
    total_recon_loss = 0.0
    
    # å‘é‡åŒ–å¤„ç†ç›¸åŒç±»å‹çš„é€šé“
    common_mask = torch.zeros(C, dtype=torch.bool, device=targets.device)
    if common_indices:
        common_mask[common_indices] = True
    
    # å¤„ç†commoné€šé“ï¼ˆæ‰¹é‡ï¼‰
    if common_mask.any():
        common_targets = targets[:, common_mask, :]  # [batch_size, n_common, T]
        common_preds = predictions[:, common_mask, :]
        
        # æ‰¹é‡è®¡ç®—æ‰€æœ‰commoné€šé“çš„æŸå¤±
        for c_idx, global_c in enumerate(torch.where(common_mask)[0]):
            target_batch = common_targets[:, c_idx, :]  # [batch_size, T]
            pred_batch = common_preds[:, c_idx, :]
            
            # æ‰¹é‡è°ƒç”¨criterion
            loss_sum = 0.0
            for b in range(batch_size):
                loss_sum += criterion(pred_batch[b], target_batch[b], channel_idx=global_c.item(), is_common=True)
            total_recon_loss += loss_sum / batch_size
    
    # å¤„ç†haveé€šé“ï¼ˆæ‰¹é‡ï¼‰
    have_mask = ~common_mask & is_real_mask[0] if is_real_mask.dim() == 2 else ~common_mask & is_real_mask
    if have_mask.any():
        have_targets = targets[:, have_mask, :]
        have_preds = predictions[:, have_mask, :]
        
        for c_idx, global_c in enumerate(torch.where(have_mask)[0]):
            target_batch = have_targets[:, c_idx, :]
            pred_batch = have_preds[:, c_idx, :]
            
            loss_sum = 0.0
            for b in range(batch_size):
                loss_sum += criterion(pred_batch[b], target_batch[b], channel_idx=global_c.item(), is_common=False)
            total_recon_loss += loss_sum / batch_size
    
    return total_recon_loss

def eval_loop(model, dataloader, criterion, device, mask_indices):
    """éªŒè¯å‡½æ•° - æ”¯æŒå¤šæ¨¡æ€æŸå¤±"""
    model.eval()
    total_loss = 0.0
    total_recon_loss = 0.0
    
    # è·å–commonæ¨¡æ€ç´¢å¼•
    common_indices = getattr(criterion, 'common_indices', [])
    
    with torch.no_grad():
        for batch_data in tqdm(dataloader, desc="Eval"):
            if len(batch_data) == 4:
                batch, _, _, is_real_mask = batch_data
            else:
                batch, _, _, is_real_mask, _ = batch_data
            
            batch = batch.to(device)
            is_real_mask = is_real_mask.to(device)
            
            masked, mask_idx = mask_channel(batch, mask_indices)
            batch_size, C, T = batch.size()
            loss = 0.0
            recon_loss = 0.0
            
            for i in range(batch_size):
                window = masked[i].t()
                out, _ = model(window)
                
                # è·å–çœŸå®é€šé“ä¿¡æ¯
                if is_real_mask.dim() == 2:
                    real_channels = is_real_mask[i]
                else:
                    real_channels = is_real_mask
                
                recon_loss_i = 0.0
                real_count = 0
                
                for c in range(C):
                    target = batch[i, c, :]
                    pred = out[c, :]
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºcommonæ¨¡æ€
                    is_common_channel = c in common_indices
                    
                    if is_common_channel:
                        # Commonæ¨¡æ€ï¼šå§‹ç»ˆè®¡ç®—æŸå¤±
                        recon_loss_i = recon_loss_i + criterion(pred, target, channel_idx=c, is_common=True)
                        real_count += 1
                    elif real_channels[c]:                        # Haveæ¨¡æ€ï¼šåªå¯¹çœŸå®é€šé“è®¡ç®—æŸå¤±
                        recon_loss_i = recon_loss_i + criterion(pred, target, channel_idx=c, is_common=False)
                        real_count += 1
                
                if real_count > 0:
                    recon_loss_i = recon_loss_i / real_count
                
                loss += recon_loss_i
                recon_loss += recon_loss_i
            
            loss = loss / batch_size
            total_loss += loss.item() * batch_size
            total_recon_loss += recon_loss
    
    n = len(dataloader.dataset)
    return total_loss / n, total_recon_loss / n, 0.0, 0.0

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='V12 å¤šæ¨¡æ€æ—¶åºç®—æ³•è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    return parser.parse_args()

# =========================
# ä¸»è®­ç»ƒå…¥å£
# =========================
def main():
    set_seed(42)
    # è®¾ç½® logging ä½¿ INFO çº§åˆ«è¾“å‡ºåˆ°æ§åˆ¶å°
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    args = parse_args()
    
    # Load config.yaml
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # å‚æ•°
    batch_size = config.get('batch_size', 32)
    epochs = config.get('epochs', 100)
    lr = config.get('lr', 1e-3)
    early_stop_patience = config.get('patience', 10)
    log_dir = config.get('log_dir', 'Logs')
    ckpt_dir = config.get('ckpt_dir', 'Checkpoints')
    mode = config.get('mode', 'train').lower()
    model_path = config.get('model_path', os.path.join(ckpt_dir, 'best_model.pth'))
    common_modalities = config.get('common_modalities', [])
    dataset_modalities_cfg = config.get('dataset_modalities', {})
    
    # ====== å¤šæ•°æ®é›†è”åˆåŠ è½½ ======
    from data import load_data, SlidingWindowDataset
    data_files = config.get('data_files', [])
    data_dir = config.get('data_dir', '')
    # æ‹¼æ¥è·¯å¾„
    data_files = [os.path.join(data_dir, f) if not os.path.isabs(f) and not os.path.exists(f) else f for f in data_files]
    all_dfs = []
    all_sources = []
    
    for f in data_files:
        df = pd.read_csv(f)
        # ä¿ç•™é‡è¦åˆ—åï¼ˆblock, Fï¼‰çš„åŸå§‹å¤§å°å†™ï¼Œå…¶ä»–åˆ—è½¬å°å†™
        original_cols = df.columns.tolist()
        new_cols = []
        for col in original_cols:
            if col in ['block', 'F', 'ID', 'Session']:  # ä¿ç•™è¿™äº›åˆ—çš„åŸå§‹å¤§å°å†™
                new_cols.append(col)
            else:
                new_cols.append(col.strip().lower())
        df.columns = new_cols
        # æ¨æ–­source
        fname = os.path.basename(f).lower()
        if 'fm' in fname:
            source = 'FM'
        elif 'od' in fname:
            source = 'OD'
        elif 'mefar' in fname:
            source = 'MEFAR'
        else:
            source = 'UNKNOWN'
        all_dfs.append(df)
        all_sources.append(source)
        print(f"åŠ è½½æ•°æ®: {f} -> {source}, shape: {df.shape}")

    # è”åˆæ•°æ®é›†æ„å»º
    combined_df = pd.concat(all_dfs, ignore_index=True)
    combined_df['source'] = [s for s, df in zip(all_sources, all_dfs) for _ in range(len(df))]    # æ£€æŸ¥è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½æ•°æ®å’Œåˆ›å»ºæ•°æ®é›† - ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬
    window_size = config.get('window_size', 320)
    step_size = config.get('step_size', 96)
    norm_method = config.get('norm_method', 'zscore')
    block_col = config.get('block_col', 'block')
    label_col = config.get('label_col', 'F')  # ä½¿ç”¨é…ç½®ä¸­çš„æ ‡ç­¾åˆ—å
    
    # æ ¹æ®é…ç½®æ–‡ä»¶è®¡ç®—æ­£ç¡®çš„ç‰¹å¾åˆ—
    common_modalities = config.get('common_modalities', [])
    dataset_modalities = config.get('dataset_modalities', {})
    
    # æ„å»ºæ‰€æœ‰ç‰¹å¾åˆ—è¡¨
    all_feature_mods = common_modalities.copy()
    for dataset, mods in dataset_modalities.items():
        have = mods.get('have', [])
        need = mods.get('need', [])
        for mod in have + need:
            if mod not in all_feature_mods:
                all_feature_mods.append(mod)
    
    # åªä½¿ç”¨é…ç½®ä¸­å®šä¹‰çš„ç‰¹å¾åˆ—ï¼Œå¿½ç•¥å…¶ä»–åˆ—
    feature_cols = all_feature_mods  # ç›´æ¥ä½¿ç”¨é…ç½®çš„åˆ—å
    print(f"ğŸ“Š ä½¿ç”¨ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
    print(f"ğŸ“Š ç‰¹å¾åˆ—: {feature_cols}")
      # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    if OPTIMIZED_LOADER_AVAILABLE and config.get('enable_fast_data_loading', False):
        print("ğŸš€ ä½¿ç”¨ä¼˜åŒ–æ•°æ®åŠ è½½å™¨")
        data_loader_factory = OptimizedDataLoader(config)
        train_ds, val_ds, test_ds = data_loader_factory.create_datasets(data_files, feature_cols)
        train_loader, val_loader, test_loader = data_loader_factory.create_data_loaders(train_ds, val_ds, test_ds)
        
        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: è®­ç»ƒ={getattr(train_ds, '__len__', lambda: 'Unknown')()}, éªŒè¯={getattr(val_ds, '__len__', lambda: 'Unknown')()}, æµ‹è¯•={getattr(test_ds, '__len__', lambda: 'Unknown')()}")
    else:
        print("âš™ï¸ ä½¿ç”¨æ ‡å‡†æ•°æ®åŠ è½½å™¨")
        # åŸæœ‰çš„æ•°æ®åŠ è½½é€»è¾‘ä½œä¸ºfallback
        from data import load_data, SlidingWindowDataset
        
        # è”åˆæ•°æ®é›†æ„å»ºï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        combined_df = pd.concat(all_dfs, ignore_index=True)
        combined_df['source'] = [s for s, df in zip(all_sources, all_dfs) for _ in range(len(df))]
        
        # åˆ›å»ºæ»‘åŠ¨çª—å£æ•°æ®é›†
        dataset = SlidingWindowDataset(
            combined_df, 
            feature_cols=feature_cols,
            window_size=window_size, 
            step_size=step_size, 
            block_col=block_col,
            label_col=label_col
        )

        print(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(dataset)}")
        check_label_distribution(dataset)

        # æ•°æ®é›†åˆ’åˆ†
        train_split = config.get('train_split', 0.6)
        val_split = (1 - train_split) / 2
        test_split = val_split

        train_size = int(train_split * len(dataset))
        val_size = int(val_split * len(dataset))
        test_size = len(dataset) - train_size - val_size

        train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])
        
        print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_ds)}, éªŒè¯é›†å¤§å°: {len(val_ds)}, æµ‹è¯•é›†å¤§å°: {len(test_ds)}")
        
        # é«˜æ€§èƒ½æ•°æ®åŠ è½½å‚æ•°
        num_workers = config.get('num_workers', 0)
        pin_memory = config.get('pin_memory', True)
        prefetch_factor = config.get('prefetch_factor', 4) if num_workers > 0 else None
        persistent_workers = config.get('persistent_workers', False) if num_workers > 0 else False
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader_kwargs = {
            'batch_size': batch_size,
            'num_workers': num_workers,
            'pin_memory': pin_memory,
            'persistent_workers': persistent_workers,
            'drop_last': config.get('dataloader_drop_last', True)
        }
        if num_workers > 0 and prefetch_factor is not None:
            dataloader_kwargs['prefetch_factor'] = prefetch_factor
        
        train_loader = DataLoader(train_ds, shuffle=True, **dataloader_kwargs)
        val_loader = DataLoader(val_ds, shuffle=False, **dataloader_kwargs)
        test_loader = DataLoader(test_ds, shuffle=False, **dataloader_kwargs)
    
    # åªä½¿ç”¨é…ç½®ä¸­å®šä¹‰çš„ç‰¹å¾åˆ—ï¼Œå¿½ç•¥å…¶ä»–åˆ—
    feature_cols = [col for col in all_feature_mods if col in combined_df.columns]
    print(f"ä½¿ç”¨ç‰¹å¾åˆ—æ•°é‡: {len(feature_cols)}")
    print(f"ç‰¹å¾åˆ—: {feature_cols}")
    
    # åˆ›å»ºæ»‘åŠ¨çª—å£æ•°æ®é›†
    dataset = SlidingWindowDataset(
        combined_df, 
        feature_cols=feature_cols,
        window_size=window_size, 
        step_size=step_size, 
        block_col=block_col,
        label_col=label_col  # æ·»åŠ label_colå‚æ•°
    )

    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
    check_label_distribution(dataset)

    # æ•°æ®é›†åˆ’åˆ†
    train_split = config.get('train_split', 0.6)
    val_split = (1 - train_split) / 2  # å‰©ä½™çš„ä¸€åŠä½œä¸ºéªŒè¯é›†
    test_split = val_split             # å¦ä¸€åŠä½œä¸ºæµ‹è¯•é›†

    train_size = int(train_split * len(dataset))
    val_size = int(val_split * len(dataset))
    test_size = len(dataset) - train_size - val_size

    train_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_ds)}, éªŒè¯é›†å¤§å°: {len(val_ds)}, æµ‹è¯•é›†å¤§å°: {len(test_ds)}")    # é«˜æ€§èƒ½æ•°æ®åŠ è½½å‚æ•° - Windowså…¼å®¹ä¼˜åŒ–
    num_workers = config.get('num_workers', 0)  # Windowså…¼å®¹ï¼šä½¿ç”¨å•è¿›ç¨‹
    pin_memory = config.get('pin_memory', True)
    prefetch_factor = config.get('prefetch_factor', 2) if num_workers > 0 else None  # å•è¿›ç¨‹æ—¶å¿…é¡»ä¸ºNone
    persistent_workers = config.get('persistent_workers', False) if num_workers > 0 else False    # åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒScaler
    use_mixed_precision = config.get('use_mixed_precision', True) and AMP_AVAILABLE and torch.cuda.is_available()
    scaler = GradScaler() if use_mixed_precision else None
      # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"é…ç½®: Batch Size={batch_size}, Mixed Precision={use_mixed_precision}, Learning Rate={lr}")
    
    if use_mixed_precision:
        print("å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (AMP)")
    else:
        print("ä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆé«˜æ€§èƒ½ç‰ˆï¼‰
    dataloader_kwargs = {
        'batch_size': batch_size,
        'num_workers': num_workers,
        'pin_memory': pin_memory,
        'persistent_workers': persistent_workers
    }
    # åªåœ¨å¤šè¿›ç¨‹æ—¶æ·»åŠ prefetch_factor
    if num_workers > 0 and prefetch_factor is not None:
        dataloader_kwargs['prefetch_factor'] = prefetch_factor
    
    train_loader = DataLoader(
        train_ds, 
        shuffle=True,
        drop_last=True,  # ä¸¢å¼ƒæœ€åä¸å®Œæ•´çš„batchä»¥ä¿æŒè®­ç»ƒç¨³å®šæ€§
        **dataloader_kwargs
    )
    val_loader = DataLoader(
        val_ds, 
        shuffle=False,
        **dataloader_kwargs
    )
    test_loader = DataLoader(
        test_ds, 
        shuffle=False,
        **dataloader_kwargs
    )

    # æ¨¡å‹å‚æ•°
    in_channels = config.get('in_channels', 32)
    hidden_channels = config.get('hidden_channels', 64)
    out_channels = config.get('out_channels', 32)
    num_classes = config.get('num_classes', 2)

    # åˆ›å»ºæ¨¡å‹
    from model import TGATUNet
    model = TGATUNet(in_channels, hidden_channels=hidden_channels, out_channels=out_channels, num_classes=num_classes)
    model.to(device)
    
    optimizer = Adam(model.parameters(), lr=lr)
    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True)
    
    # åˆ›å»ºæŸå¤±å‡½æ•°
    if config.get('loss_config', {}).get('type') == 'multimodal':
        criterion = create_simple_multimodal_criterion(config)
        print("ä½¿ç”¨å¤šæ¨¡æ€æŸå¤±å‡½æ•°")
    else:
        criterion = MSELoss()
        print("ä½¿ç”¨æ ‡å‡†MSEæŸå¤±å‡½æ•°")    # åˆ›å»ºå¢å¼ºéªŒè¯ç®¡ç†å™¨
    log_dir_full = os.path.join(log_dir, datetime.now().strftime('%Y%m%d_%H%M%S'))
    os.makedirs(log_dir_full, exist_ok=True)
    os.makedirs(ckpt_dir, exist_ok=True)
    
    enhanced_val_manager = EnhancedValidationManager(
        patience=early_stop_patience,
        min_delta=float(config.get('enhanced_validation', {}).get('min_delta', 1e-6)),
        save_dir=os.path.join(log_dir_full, 'enhanced_validation')    )
    print(f"ğŸ“Š å¯ç”¨å¢å¼ºéªŒè¯ç­–ç•¥ï¼Œè€å¿ƒåº¦={early_stop_patience}")

    # TensorBoard æ—¥å¿—å™¨åˆå§‹åŒ–
    writer = SummaryWriter(log_dir=log_dir_full)
      # Mask indices (ç¤ºä¾‹)
    mask_indices = [0, 1, 2]  # å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´

    # ========== è®­ç»ƒæ¨¡å¼ ========== 
    if mode == 'train':
        print("ğŸ¯ ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼ + å¢å¼ºéªŒè¯ç­–ç•¥")
        for epoch in range(1, epochs + 1):            # è·å–æ¢¯åº¦ç´¯ç§¯è®¾ç½®
            accumulate_grad_batches = config.get('accumulate_grad_batches', 2)
            
            train_loss, train_recon, train_cls, train_encode_acc, train_decode_acc = train_phased_with_grad_accumulation(
                model, train_loader, optimizer, criterion, device, mask_indices, 
                accumulate_grad_batches=accumulate_grad_batches,
                use_mixed_precision=use_mixed_precision, scaler=scaler
            )
            
            # ä½¿ç”¨å¢å¼ºéªŒè¯ç­–ç•¥
            if enhanced_val_manager.should_validate(epoch):
                # è®¡ç®—å¢å¼ºéªŒè¯æŒ‡æ ‡
                val_metrics = enhanced_val_manager.compute_enhanced_validation_metrics(
                    model, val_loader, criterion, device, mask_indices
                )
                
                # æ›´æ–°æŒ‡æ ‡å¹¶è·å–æ—©åœå»ºè®®
                early_stop_info = enhanced_val_manager.update_metrics(val_metrics, epoch)
                
                # è®°å½•ç»“æœ
                enhanced_val_manager.log_validation_results(val_metrics, epoch, early_stop_info)
                
                # å­¦ä¹ ç‡è°ƒåº¦
                scheduler.step(val_metrics['val_loss'])
                
                # è·å–å½“å‰å­¦ä¹ ç‡
                current_lr = optimizer.param_groups[0]['lr']
                
                # å¢å¼ºæ—¥å¿—è®°å½•
                logging.info(f"[ENHANCED_TRAIN] Epoch {epoch}: "
                           f"train_loss={train_loss:.6f}, train_encode_acc={train_encode_acc:.4f}, train_decode_acc={train_decode_acc:.4f}, "
                           f"val_loss={val_metrics['val_loss']:.6f}, val_encode_acc={val_metrics['val_encode_accuracy']:.4f}, "
                           f"val_decode_acc={val_metrics['val_decode_accuracy']:.4f}, val_f1={val_metrics['val_f1_score']:.4f}, "
                           f"common_recon={val_metrics['val_common_recon_loss']:.6f}, "
                           f"have_recon={val_metrics['val_have_recon_loss']:.6f}, "
                           f"lr={current_lr:.6e}")
                
                # TensorBoardè®°å½•å¢å¼ºæŒ‡æ ‡
                writer.add_scalar('Train/Loss/train', train_loss, epoch)
                writer.add_scalar('Train/Recon/train', train_recon, epoch)
                writer.add_scalar('Train/Cls/train', train_cls, epoch)
                writer.add_scalar('Train/Encode_Acc/train', train_encode_acc, epoch)
                writer.add_scalar('Train/Decode_Acc/train', train_decode_acc, epoch)
                writer.add_scalar('Enhanced_Val/Loss', val_metrics['val_loss'], epoch)
                writer.add_scalar('Enhanced_Val/Accuracy', val_metrics['val_accuracy'], epoch)
                writer.add_scalar('Enhanced_Val/Encode_Accuracy', val_metrics['val_encode_accuracy'], epoch)
                writer.add_scalar('Enhanced_Val/Decode_Accuracy', val_metrics['val_decode_accuracy'], epoch)
                writer.add_scalar('Enhanced_Val/F1_Score', val_metrics['val_f1_score'], epoch)
                writer.add_scalar('Enhanced_Val/Precision', val_metrics['val_precision'], epoch)
                writer.add_scalar('Enhanced_Val/Recall', val_metrics['val_recall'], epoch)
                writer.add_scalar('Enhanced_Val/Common_Recon', val_metrics['val_common_recon_loss'], epoch)
                writer.add_scalar('Enhanced_Val/Have_Recon', val_metrics['val_have_recon_loss'], epoch)
                writer.add_scalar('Train/LR', current_lr, epoch)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if epoch == enhanced_val_manager.best_epoch:
                    torch.save(model.state_dict(), os.path.join(ckpt_dir, 'best_model.pth'))
                    torch.save(model.state_dict(), model_path)
                    logging.info(f"Saved best model at epoch {epoch} (composite score: {early_stop_info['best_composite_score']:.6f})")
                
                # æ—©åœæ£€æŸ¥
                if early_stop_info['should_stop']:
                    logging.info(f"Enhanced early stopping triggered at epoch {epoch}")
                    break
                    
                # è¿‡æ‹Ÿåˆè­¦å‘Š
                if early_stop_info['is_overfitting']:
                    logging.warning(f"[OVERFITTING_DETECTED] Epoch {epoch}: æ£€æµ‹åˆ°å¯èƒ½çš„è¿‡æ‹Ÿåˆ")
            
            else:
                # ä¸éœ€è¦éªŒè¯çš„epochï¼Œåªåšç®€å•çš„æ—¥å¿—è®°å½•
                current_lr = optimizer.param_groups[0]['lr']
                logging.info(f"[TRAIN] Epoch {epoch}: train_loss={train_loss:.6f}, train_encode_acc={train_encode_acc:.4f}, train_decode_acc={train_decode_acc:.4f}, lr={current_lr:.6e}")
                writer.add_scalar('Train/Loss/train', train_loss, epoch)
                writer.add_scalar('Train/Encode_Acc/train', train_encode_acc, epoch)
                writer.add_scalar('Train/Decode_Acc/train', train_decode_acc, epoch)
                writer.add_scalar('Train/LR', current_lr, epoch)
            
            # === åŠ¨æ€needè¡¥å…¨ ===
            # æ³¨æ„ï¼šä¸ºé¿å…æ•°æ®æ³„éœ²ï¼Œåªåœ¨è®­ç»ƒæ¨¡å¼ä¸‹å¯¹è®­ç»ƒé›†è¿›è¡ŒåŠ¨æ€æ›´æ–°
            if hasattr(train_ds, 'dataset') and hasattr(train_ds.dataset, 'update_need'):
                # å¦‚æœæ˜¯Subset/RandomSplitåŒ…è£…ï¼Œå–åŸå§‹dataset
                complete_need_with_model(model, train_ds.dataset, device)
            elif hasattr(train_ds, 'update_need'):
                complete_need_with_model(model, train_ds, device)
        
        # è®­ç»ƒç»“æŸåä¿å­˜å¢å¼ºéªŒè¯å¯è§†åŒ–
        enhanced_val_manager.save_validation_plots()
        best_summary = enhanced_val_manager.get_best_metrics_summary()
        logging.info(f"Training completed. Best metrics: {best_summary}")
          # è‡ªåŠ¨è¯„ä¼° - ä½¿ç”¨å®Œæ•´çš„å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        latest_model = model_path if os.path.exists(model_path) else os.path.join(ckpt_dir, 'best_model.pth')
        if latest_model and os.path.exists(latest_model):
            model.load_state_dict(torch.load(latest_model))
            logging.info(f"[Test Evaluation] å¼€å§‹å¯¹æµ‹è¯•é›†è¿›è¡Œå®Œæ•´è¯„ä¼°...")
            
            # ä½¿ç”¨å¢å¼ºéªŒè¯ç®¡ç†å™¨è®¡ç®—å®Œæ•´çš„æµ‹è¯•é›†æŒ‡æ ‡
            test_metrics = enhanced_val_manager.compute_enhanced_validation_metrics(
                model, test_loader, criterion, device, mask_indices
            )
            
            # è¯¦ç»†è¾“å‡ºæµ‹è¯•é›†è¯„ä¼°ç»“æœ
            logging.info(f"[TEST_RESULTS] æµ‹è¯•é›†å®Œæ•´è¯„ä¼°ç»“æœ:")
            logging.info(f"  - Test Loss: {test_metrics['val_loss']:.6f}")
            logging.info(f"  - Test Accuracy: {test_metrics['val_accuracy']:.4f}")
            logging.info(f"  - Test F1 Score: {test_metrics['val_f1_score']:.4f}")
            logging.info(f"  - Test Precision: {test_metrics['val_precision']:.4f}")
            logging.info(f"  - Test Recall: {test_metrics['val_recall']:.4f}")
            logging.info(f"  - Test Common Recon Loss: {test_metrics['val_common_recon_loss']:.6f}")
            logging.info(f"  - Test Have Recon Loss: {test_metrics['val_have_recon_loss']:.6f}")
            logging.info(f"  - Test Classification Loss: {test_metrics['val_cls_loss']:.6f}")
            
            # è®°å½•åˆ°TensorBoard
            writer.add_scalar('Test/Loss', test_metrics['val_loss'])
            writer.add_scalar('Test/Accuracy', test_metrics['val_accuracy'])
            writer.add_scalar('Test/F1_Score', test_metrics['val_f1_score'])
            writer.add_scalar('Test/Precision', test_metrics['val_precision'])
            writer.add_scalar('Test/Recall', test_metrics['val_recall'])
            writer.add_scalar('Test/Common_Recon_Loss', test_metrics['val_common_recon_loss'])
            writer.add_scalar('Test/Have_Recon_Loss', test_metrics['val_have_recon_loss'])
            writer.add_scalar('Test/Cls_Loss', test_metrics['val_cls_loss'])
        else:
            logging.warning(f"æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•è¯„ä¼°ï¼(æŸ¥æ‰¾è·¯å¾„: {latest_model})")
    
    elif mode == 'eval':
        # è¯„ä¼°æ¨¡å¼ - ä½¿ç”¨å®Œæ•´çš„å¢å¼ºè¯„ä¼°æŒ‡æ ‡
        latest_model = model_path if os.path.exists(model_path) else os.path.join(ckpt_dir, 'best_model.pth')
        if latest_model and os.path.exists(latest_model):
            model.load_state_dict(torch.load(latest_model))
            logging.info(f"[Eval Mode] åŠ è½½æ¨¡å‹: {latest_model}")
            logging.info(f"[Eval Mode] å¼€å§‹å¯¹æµ‹è¯•é›†è¿›è¡Œå®Œæ•´è¯„ä¼°...")
              # åˆ›å»ºä¸´æ—¶çš„å¢å¼ºéªŒè¯ç®¡ç†å™¨ç”¨äºè¯„ä¼°æ¨¡å¼
            temp_enhanced_val_manager = EnhancedValidationManager(
                patience=early_stop_patience,
                min_delta=float(config.get('enhanced_validation', {}).get('min_delta', 1e-6)),
                save_dir=os.path.join(log_dir_full, 'eval_mode_validation')
            )
            
            # ä½¿ç”¨å¢å¼ºéªŒè¯ç®¡ç†å™¨è®¡ç®—å®Œæ•´çš„æµ‹è¯•é›†æŒ‡æ ‡
            test_metrics = temp_enhanced_val_manager.compute_enhanced_validation_metrics(
                model, test_loader, criterion, device, mask_indices
            )
            
            # è¯¦ç»†è¾“å‡ºè¯„ä¼°ç»“æœ
            logging.info(f"[EVAL_RESULTS] è¯„ä¼°æ¨¡å¼å®Œæ•´ç»“æœ:")
            logging.info(f"  - Test Loss: {test_metrics['val_loss']:.6f}")
            logging.info(f"  - Test Accuracy: {test_metrics['val_accuracy']:.4f}")
            logging.info(f"  - Test F1 Score: {test_metrics['val_f1_score']:.4f}")
            logging.info(f"  - Test Precision: {test_metrics['val_precision']:.4f}")
            logging.info(f"  - Test Recall: {test_metrics['val_recall']:.4f}")
            logging.info(f"  - Test Common Recon Loss: {test_metrics['val_common_recon_loss']:.6f}")
            logging.info(f"  - Test Have Recon Loss: {test_metrics['val_have_recon_loss']:.6f}")
            logging.info(f"  - Test Classification Loss: {test_metrics['val_cls_loss']:.6f}")
            
            # è®°å½•åˆ°TensorBoard
            writer.add_scalar('Eval/Loss', test_metrics['val_loss'])
            writer.add_scalar('Eval/Accuracy', test_metrics['val_accuracy'])
            writer.add_scalar('Eval/F1_Score', test_metrics['val_f1_score'])
            writer.add_scalar('Eval/Precision', test_metrics['val_precision'])
            writer.add_scalar('Eval/Recall', test_metrics['val_recall'])
            writer.add_scalar('Eval/Common_Recon_Loss', test_metrics['val_common_recon_loss'])
            writer.add_scalar('Eval/Have_Recon_Loss', test_metrics['val_have_recon_loss'])
            writer.add_scalar('Eval/Cls_Loss', test_metrics['val_cls_loss'])
        else:
            logging.warning(f"è¯„ä¼°æ¨¡å¼ä¸‹æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹ï¼(æŸ¥æ‰¾è·¯å¾„: {latest_model}")

if __name__ == "__main__":
    main()
